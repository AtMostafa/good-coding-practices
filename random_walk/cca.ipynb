{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### part0:imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M1_spikes\n",
      "84\n",
      "PMd_spikes\n",
      "211\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "from tqdm import tqdm\n",
    "import os, sys, pathlib\n",
    "from pprint import pprint\n",
    "import gc, time\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import logging, warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import PyPDF2 as ppdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.linalg as linalg\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from scipy.stats import wilcoxon, mannwhitneyu, linregress\n",
    "import math\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "import matplotlib.lines as mlines\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter\n",
    "from matplotlib.collections import LineCollection\n",
    "from collections import Counter\n",
    "import torch\n",
    "\n",
    "import pyaldata as pyal\n",
    "\n",
    "if \"__file__\" not in dir():\n",
    "    try:\n",
    "        NBPath = pathlib.Path.cwd()\n",
    "        RepoPath = NBPath.parent\n",
    "        os.chdir(RepoPath)\n",
    "\n",
    "        from tools import utilityTools as utility\n",
    "        from tools import dataTools as dt\n",
    "        import params\n",
    "        reload(params)\n",
    "        defs = params.random_walk_defs\n",
    "\n",
    "        set_rc =  params.set_rc_params\n",
    "        set_rc()\n",
    "        root = params.root\n",
    "\n",
    "        os.chdir(RepoPath / 'random_walk')\n",
    "        %run \"_dataset-selection.ipynb\"\n",
    "\n",
    "    finally:\n",
    "        os.chdir(NBPath)\n",
    "    print('Done')\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Get data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_full_random_walk_data():\n",
    "    full_list_MCx = []\n",
    "    for animal, sessionList in GoodDataList['MCx'].items():\n",
    "        full_list_MCx.append((animal,sessionList))\n",
    "    full_list_MCx = [(animal,session) for animal,sessions in full_list_MCx for session in set(sessions)]\n",
    "    # load the DFs\n",
    "    allDFs_MCx = []\n",
    "    allDFs_exec_MCx = []\n",
    "    for animal, session in full_list_MCx:\n",
    "        path = root/'random_walk'/animal/session\n",
    "        df_ = defs.prep_general(dt.load_pyal_data(path))\n",
    "\n",
    "        #separate into reaches\n",
    "        df_ = defs.get_reaches_df(df_)\n",
    "        df_['reach_id'] = range(len(df_))\n",
    "\n",
    "        #subset center-out trials\n",
    "        df_ = df_[df_.center_dist < defs.subset_radius]\n",
    "        df_ = df_.reset_index()\n",
    "\n",
    "        #execution epoch\n",
    "        for col in df_.columns:  #TODO: placeholder to prevent bug in pyaldata\n",
    "            if 'unit_guide' in col:\n",
    "                df_ = df_.drop([col], axis = 1)\n",
    "        df_ = pyal.add_movement_onset(df_)\n",
    "        allDFs_MCx.append(df_)\n",
    "\n",
    "        df_ = pyal.restrict_to_interval(df_, epoch_fun=defs.exec_epoch)\n",
    "        allDFs_exec_MCx.append(df_)\n",
    "\n",
    "        # print(len(df_))\n",
    "        \n",
    "    return full_list_MCx, allDFs_MCx, allDFs_exec_MCx\n",
    "\n",
    "def get_paired_dfs():\n",
    "    MCx_list, allDFs_MCx, allDFs_exec_MCx = get_full_random_walk_data()\n",
    "\n",
    "    ref_file = 'Chewie_RT_CS_2016-10-21.mat'\n",
    "    ref_i = [y for x,y in MCx_list].index(ref_file)\n",
    "    df1 = allDFs_exec_MCx[ref_i]\n",
    "\n",
    "    Mihili_files = GoodDataList['MCx']['Mihili']\n",
    "    MrT_files = GoodDataList['MCx']['MrT']\n",
    "    comparison_files = Mihili_files + MrT_files\n",
    "\n",
    "    paired_dfs = []\n",
    "    for ex_file in comparison_files:\n",
    "        # print(ex_file)\n",
    "\n",
    "        ex_i = [y for x,y in MCx_list].index(ex_file)\n",
    "        df2 = allDFs_exec_MCx[ex_i]\n",
    "\n",
    "        #subset dataframes with matched reaches\n",
    "        df1_idx, df2_idx = defs.get_matched_reaches_idx(df1, df2)\n",
    "        df1_subset = df1.iloc[df1_idx]\n",
    "        df2_subset = df2.iloc[df2_idx]\n",
    "\n",
    "        #get dataframes from whole-trial data\n",
    "        df1_ = pd.DataFrame({'reach_id':df1_subset.reach_id}).merge(allDFs_MCx[ref_i])\n",
    "        df2_ = pd.DataFrame({'reach_id':df2_subset.reach_id}).merge(allDFs_MCx[ex_i])\n",
    "\n",
    "        #set target ids\n",
    "        # print(len(df1_) - (df1_.target_group.values == df2_.target_group.values).sum(), 'diff target groups')\n",
    "        df1_.target_group = df2_.target_group.values \n",
    "        df1_['target_id'] = df1_.target_group.values\n",
    "        df2_['target_id'] = df2_.target_group.values\n",
    "\n",
    "        #only keep target groups with enough trials\n",
    "        counter = Counter(df1_.target_group)\n",
    "        subset_target_groups = [k for k, c in counter.items() if c >= defs.min_trials_per_target]\n",
    "        df1_ = df1_[df1_.target_group.isin(subset_target_groups)]\n",
    "        df2_ = df2_[df2_.target_group.isin(subset_target_groups)]\n",
    "        # print(len(subset_target_groups))\n",
    "\n",
    "        paired_dfs.append((ex_file, df1_, df2_))\n",
    "    \n",
    "    return paired_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len(paired_dfs)=12\n"
     ]
    }
   ],
   "source": [
    "MCx_list, allDFs_MCx, allDFs_exec_MCx = get_full_random_walk_data()\n",
    "paired_dfs = get_paired_dfs()\n",
    "print(f'{len(paired_dfs)=}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform CCA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "allCCs=[]\n",
    "CCsL=[]\n",
    "CCsU=[]\n",
    "\n",
    "for ex_file, df1_, df2_ in paired_dfs:\n",
    "    subset_target_groups = np.unique(df1_.target_group)\n",
    "\n",
    "    #perform cca ################################################\n",
    "    AllData1, AllData2 = defs.get_paired_data_arrays(df1_, df2_, epoch = defs.exec_epoch, area=defs.areas[2], model=defs.n_components)\n",
    "    # print(AllData1.shape) #session x target_groups x trials x time x modes\n",
    "    _,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "    \n",
    "    for sessionData1,sessionData2 in zip(AllData1,AllData2):\n",
    "        data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "        data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "        allCCs.append(dt.canoncorr(data1, data2))\n",
    "    # allCCs = np.array(allCCs).T\n",
    "        \n",
    "    # lower bound\n",
    "    len_trial = int(np.round(np.diff(defs.WINDOW_exec)/defs.BIN_SIZE))\n",
    "    AllDataL1 = defs._get_data_array(df1_, epoch_L=len_trial, area=defs.areas[2], model=defs.n_components)\n",
    "    AllDataL2 = defs._get_data_array(df2_, epoch_L=len_trial, area=defs.areas[2], model=defs.n_components)\n",
    "    _,_, min_trials, min_time,_ = np.min((AllDataL1.shape,AllDataL2.shape),axis=0)\n",
    "    \n",
    "    for sessionData1,sessionData2 in zip(AllDataL1,AllDataL2):\n",
    "        r = []\n",
    "        for n in range(params.n_iter * 10):\n",
    "            sessionData1_sh = params.rng.permutation(sessionData1,axis=0)\n",
    "            sessionData2_sh = params.rng.permutation(sessionData2,axis=0)\n",
    "            data1 = np.reshape(sessionData1_sh[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(sessionData2_sh[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            r.append(dt.canoncorr(data1, data2))\n",
    "        CCsL.append(r)\n",
    "        \n",
    "    # upper bound\n",
    "    n_shared_trial1 = AllData1.shape[2]\n",
    "    trialList1 = np.arange(n_shared_trial1)\n",
    "    \n",
    "    for session, sessionData in enumerate([AllData1[0], AllData2[0]]):\n",
    "        r = []\n",
    "        for n in range(params.n_iter * 10):\n",
    "            params.rng.shuffle(trialList1)\n",
    "            # non-overlapping randomised trials\n",
    "            trial1 = trialList1[:n_shared_trial1//2]\n",
    "            trial2 = trialList1[-(n_shared_trial1//2):]\n",
    "            data1 = np.reshape(sessionData[:,trial1,:,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(sessionData[:,trial2,:,:], (-1,defs.n_components))\n",
    "            r.append(dt.canoncorr(data1, data2))\n",
    "        CCsU.append(r)\n",
    "\n",
    "allCCs = np.array(allCCs).T\n",
    "CCsL = np.array(CCsL)\n",
    "CCsL = np.percentile(CCsL, 1, axis=1).T\n",
    "CCsU = np.array(CCsU)\n",
    "CCsU = np.percentile(CCsU, 99, axis=1).T\n",
    "\n",
    "\n",
    "# plotting\n",
    "fig, ax = plt.subplots()\n",
    "set_rc()\n",
    "x_ = np.arange(1,defs.n_components+1)\n",
    "utility.shaded_errorbar(ax, x_, allCCs, color=params.colors.MainCC, marker = 'o')\n",
    "utility.shaded_errorbar(ax, x_, CCsU, color=params.colors.UpperCC, marker = '<', ls='--')\n",
    "utility.shaded_errorbar(ax, x_, CCsL, color=params.colors.LowerCC, marker = '>', ls=':')\n",
    "\n",
    "ax.set_ylim([-.05,1])\n",
    "ax.set_xlim([.6, defs.n_components+.6])\n",
    "ax.set_xlabel('Neural mode')\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.set_ylabel('Canonical correlation')\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.spines['bottom'].set_bounds([1,defs.n_components])\n",
    "ax.spines['left'].set_bounds([0,1])\n",
    "ax.text(x=defs.n_components, y=1, s= f'$n={CCsL.shape[1]}$ pairs of sessions\\nacross $3$ monkeys',\n",
    "        ha='right', va='top', fontsize=mpl.rcParams['xtick.labelsize'])\n",
    "\n",
    "fig.savefig(params.figPath / 'RW-CCA.pdf', format='pdf', bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subset targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "redo = False\n",
    "import random\n",
    "import itertools\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "subset_CCs = []\n",
    "for ex_file, df1_, df2_ in paired_dfs:\n",
    "    # if ex_file != 'Mihili_RT_VR_2014-01-14.mat': continue\n",
    "\n",
    "    #perform cca ################################################\n",
    "    AllData1, AllData2 = defs.get_paired_data_arrays(df1_, df2_, epoch = defs.exec_epoch, area=defs.areas[2], model=defs.n_components)\n",
    "    # print(AllData1.shape) #session x target_groups x trials x time x modes\n",
    "    _,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "    allCCs=[]\n",
    "    for sessionData1,sessionData2 in zip(AllData1,AllData2):\n",
    "        data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "        data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "        allCCs.append(dt.canoncorr(data1, data2))\n",
    "    allCCs = np.array(allCCs).T\n",
    "\n",
    "    #get saved ccs\n",
    "    info = '_'.join([ex_file[:-4], str(defs.n_angle_groups), str(defs.min_trials_per_target), str(defs.match_mse_cutoff_perc)])\n",
    "    pathPickle = root / 'random-walk-pickles'/ f'cca_subset_targets_{info}.p'\n",
    "\n",
    "    if os.path.exists(pathPickle) and not redo:\n",
    "        #load file\n",
    "        with open(pathPickle, 'rb') as f:\n",
    "            allCCs_subset = pickle.load(f)\n",
    "    else: \n",
    "        target_groups = np.unique(df1_.target_group)\n",
    "        n_target_groups = len(target_groups)\n",
    "\n",
    "        #remove n target groups\n",
    "        allCCs_subset = []\n",
    "        for n in tqdm(range(2,n_target_groups)):\n",
    "            allCCs_subset_ = []\n",
    "            comb = list(itertools.combinations(range(n_target_groups), n))\n",
    "            # print(n, len(comb))\n",
    "            comb = random.sample(comb, min(len(comb),10000)) #do max 10000 combos\n",
    "            for target_idx in comb:\n",
    "                AllData1_ = AllData1[:, target_idx,:,:,:]\n",
    "                AllData2_ = AllData2[:, target_idx,:,:,:]\n",
    "\n",
    "                for sessionData1,sessionData2 in zip(AllData1_,AllData2_):\n",
    "                    data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "                    data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "                    allCCs_subset_.append(dt.canoncorr(data1, data2))\n",
    "            allCCs_subset.append(np.array(allCCs_subset_).T) \n",
    "        \n",
    "        with open(pathPickle, 'wb') as f:\n",
    "            pickle.dump(allCCs_subset, f)   \n",
    "\n",
    "    subset_CCs.append(allCCs_subset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Plot CCs when subsetting targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "session_colors = utility.get_colors(len(subset_CCs))\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for i, allCCs_subset in enumerate(subset_CCs):\n",
    "    all_mean_ccs = [np.mean(x[:4],axis = 0) for x in allCCs_subset]\n",
    "    mean_ccs = [np.mean(x) for x in all_mean_ccs]\n",
    "\n",
    "    n_target_groups = len(mean_ccs)+2\n",
    "    x = range(2, n_target_groups)\n",
    "    colors = utility.get_colors(n_target_groups)\n",
    "\n",
    "    ax.plot(x, mean_ccs, c = 'grey')\n",
    "\n",
    "    plt.xlabel('Number of targets')\n",
    "    plt.ylabel('Canonical correlation')\n",
    "    plt.ylim([0,1])\n",
    "    plt.xlim([1,28])\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_bounds([2,30])\n",
    "    ax.spines['left'].set_bounds([0,1])\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "\n",
    "    \n",
    "info = '_'.join([str(defs.n_angle_groups), str(defs.min_trials_per_target), str(defs.match_mse_cutoff_perc)])\n",
    "# plt.savefig(\"figures/mean_cca_subset_targets_\"+info+'.pdf', format= 'pdf')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### subset neurons by dropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_neurons(df, n_removed, signal):\n",
    "    df_ = df.copy()\n",
    "    n_neurons = df_[signal].values[0].shape[1]\n",
    "    size = n_neurons - n_removed\n",
    "    if size < defs.n_components:\n",
    "        return False\n",
    "\n",
    "    mask = params.rng.choice(n_neurons, size, replace = False)\n",
    "    df_[signal] = [arr[:, mask] for arr in df_[signal]]\n",
    "    \n",
    "    return df_\n",
    "\n",
    "n_per_removal = 10\n",
    "\n",
    "def plot_cca_subsample_neurons(df1, df2):\n",
    "\n",
    "    #successively remove neurons\n",
    "    signal = defs.areas[2] + '_rates'\n",
    "    n_neurons1 = df1[signal].values[0].shape[1]\n",
    "    n_neurons2 = df2[signal].values[0].shape[1]\n",
    "    n_removals = math.floor((min(n_neurons1,n_neurons2)-defs.n_components)/n_per_removal)\n",
    "    n_iter = 50\n",
    "\n",
    "    CC_corr_subsample = []\n",
    "    for i in tqdm(range(n_removals+1)):\n",
    "        #remove neurons\n",
    "        n_removed = i*n_per_removal\n",
    "        df1_ = remove_neurons(df1, n_removed, signal)\n",
    "        df2_ = remove_neurons(df2, n_removed, signal)\n",
    "        if df1_ is False or df2_ is False:\n",
    "            continue\n",
    "\n",
    "        CC_corr=[]\n",
    "        for j in range(n_iter):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                AllData1, AllData2 = defs.get_paired_data_arrays(df1_, df2_, epoch = defs.exec_epoch, area=defs.areas[2], model=defs.n_components)\n",
    "    \n",
    "            *_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "            data1 = np.reshape(AllData1[0,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(AllData2[0,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            CC_corr.append(dt.canoncorr(data1, data2)[:4].mean())\n",
    "        CC_corr_subsample.append(np.mean(CC_corr))\n",
    "    \n",
    "    return CC_corr_subsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC_corr_subsample= []\n",
    "for _, df1_, df2_ in paired_dfs:\n",
    "    CC_corr_subsample.append( plot_cca_subsample_neurons(df1_,df2_))\n",
    "\n",
    "set_rc()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "\n",
    "for cc in CC_corr_subsample:\n",
    "    ax.plot(np.arange(len(cc))*n_per_removal, np.array(cc), color=params.colors.MainCC, marker = 'o')\n",
    "\n",
    "ax.set_xlabel('Number of dropped neurons')\n",
    "ax.set_ylabel('Canonical correlation')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlim([-.5*n_per_removal, max([len(cc) for cc in CC_corr_subsample])*n_per_removal])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('$%0.2f$'))\n",
    "\n",
    "fig.savefig(params.figPath / 'RW-neuron-drop.pdf', format='pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "subset neurons by keeping _n_ units"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def keep_neurons(df, n_keep, signal):\n",
    "    df_ = df.copy()\n",
    "    n_neurons = df_[signal].values[0].shape[1]\n",
    "    if n_neurons < n_keep:\n",
    "        return False\n",
    "    mask = params.rng.choice(n_neurons, n_keep, replace = False)\n",
    "    df_[signal] = [arr[:, mask] for arr in df_[signal]]\n",
    "    \n",
    "    return df_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_step = 10\n",
    "\n",
    "def plot_cca_sub_neurons_inc(df1, df2):\n",
    "    #successively keep more neurons\n",
    "    signal = defs.areas[2] + '_rates'\n",
    "\n",
    "    n_neurons1 = df1[signal].values[0].shape[1]\n",
    "    n_neurons2 = df2[signal].values[0].shape[1]\n",
    "    \n",
    "    n_iter = 50\n",
    "\n",
    "    CC_corr_subsample = []\n",
    "    for n_keep in range(defs.n_components+1, 1000, n_step):\n",
    "        #remove neurons\n",
    "        df1_ = keep_neurons(df1, n_keep, signal)\n",
    "        df2_ = keep_neurons(df2, n_keep, signal)\n",
    "        if df1_ is False or df2_ is False:\n",
    "            break\n",
    "\n",
    "        CC_corr=[]\n",
    "        for j in range(n_iter):\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\")\n",
    "                AllData1, AllData2 = defs.get_paired_data_arrays(df1_, df2_, epoch = defs.exec_epoch, area=defs.areas[2], model=defs.n_components)\n",
    "    \n",
    "            *_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "            data1 = np.reshape(AllData1[0,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(AllData2[0,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            CC_corr.append(dt.canoncorr(data1, data2)[:4].mean())\n",
    "        CC_corr_subsample.append(np.mean(CC_corr))\n",
    "    \n",
    "    return CC_corr_subsample\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "CC_corr_subsample= []\n",
    "for _, df1_, df2_ in paired_dfs:\n",
    "    CC_corr_subsample.append(plot_cca_sub_neurons_inc(df1_,df2_))\n",
    "\n",
    "# Plotting\n",
    "set_rc()\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "for cc in CC_corr_subsample:\n",
    "    ax.plot((np.arange(len(cc))*n_step)+defs.n_components, np.array(cc), color=params.colors.MainCC, marker = 'o')\n",
    "\n",
    "ax.set_xlabel('Number of included neurons')\n",
    "ax.set_ylabel('Canonical correlation')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlim([-.5*n_per_removal, max([len(cc) for cc in CC_corr_subsample])*n_per_removal+5])\n",
    "ax.spines['top'].set_visible(False)\n",
    "ax.spines['right'].set_visible(False)\n",
    "ax.yaxis.set_major_formatter(FormatStrFormatter('$%0.2f$'))\n",
    "\n",
    "fig.savefig(params.figPath / 'RW-neuron-keep.pdf', format='pdf', bbox_inches='tight')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### permutated targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#test controls with shuffled targets\n",
    "for ex_file, df1_, df2_ in paired_dfs[:2]:\n",
    "    # df1_ = df1_.sample(frac = 1).reset_index() #shuffle rows\n",
    "    # df1_.target_id = np.random.permutation(df1_.target_id.values) #shuffle all targets: lowers\n",
    "    subset_target_groups = np.unique(df1_.target_group)\n",
    "    \n",
    "    #perform cca ################################################\n",
    "    AllData1, AllData2 = defs.get_paired_data_arrays(df1_, df2_, epoch = defs.exec_epoch, area=defs.areas[2], model=defs.n_components)\n",
    "    shuffled_idx = np.random.permutation(np.arange(AllData1.shape[1])) #permutate targets: lowers\n",
    "    AllData1[:] = AllData1[:,shuffled_idx,:,:,:]\n",
    "\n",
    "    # print(AllData1.shape) #session x target_groups x trials x time x modes\n",
    "\n",
    "    _,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "    allCCs=[]\n",
    "    for sessionData1,sessionData2 in zip(AllData1,AllData2):\n",
    "        data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "        data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "        allCCs.append(dt.canoncorr(data1, data2))\n",
    "    allCCs = np.array(allCCs).T\n",
    "        \n",
    "    # lower bound\n",
    "    len_trial = int(np.round(np.diff(defs.WINDOW_exec)/defs.BIN_SIZE))\n",
    "    AllDataL1 = defs._get_data_array(df1_, epoch_L=len_trial, area=defs.areas[2], model=defs.n_components)\n",
    "    AllDataL2 = defs._get_data_array(df2_, epoch_L=len_trial, area=defs.areas[2], model=defs.n_components)\n",
    "    _,_, min_trials, min_time,_ = np.min((AllDataL1.shape,AllDataL2.shape),axis=0)\n",
    "    CCsL=[]\n",
    "    for sessionData1,sessionData2 in zip(AllDataL1,AllDataL2):\n",
    "        r = []\n",
    "        for n in range(params.n_iter * 10):\n",
    "            sessionData1_sh = params.rng.permutation(sessionData1,axis=0)\n",
    "            sessionData2_sh = params.rng.permutation(sessionData2,axis=0)\n",
    "            data1 = np.reshape(sessionData1_sh[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(sessionData2_sh[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            r.append(dt.canoncorr(data1, data2))\n",
    "        CCsL.append(r)\n",
    "    CCsL = np.array(CCsL)\n",
    "    CCsL = np.percentile(CCsL, 1, axis=1).T\n",
    "        \n",
    "    # upper bound\n",
    "    n_shared_trial1 = AllData1.shape[2]\n",
    "    trialList1 = np.arange(n_shared_trial1)\n",
    "    CCsU=[]\n",
    "    for session, sessionData in enumerate([AllData1[0], AllData2[0]]):\n",
    "        r = []\n",
    "        for n in range(params.n_iter * 10):\n",
    "            params.rng.shuffle(trialList1)\n",
    "            # non-overlapping randomised trials\n",
    "            trial1 = trialList1[:n_shared_trial1//2]\n",
    "            trial2 = trialList1[-(n_shared_trial1//2):]\n",
    "            data1 = np.reshape(sessionData[:,trial1,:,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(sessionData[:,trial2,:,:], (-1,defs.n_components))\n",
    "            r.append(dt.canoncorr(data1, data2))\n",
    "        CCsU.append(r)\n",
    "    CCsU = np.array(CCsU)\n",
    "    CCsU = np.percentile(CCsU, 99, axis=1).T\n",
    "\n",
    "    # plotting\n",
    "    fig, ax = plt.subplots()\n",
    "    x_ = np.arange(1,defs.n_components+1)\n",
    "    ax.plot(x_, allCCs, color=params.colors.MainCC, marker = 'o', label=f'Across monkeys')\n",
    "    ax.plot(x_, CCsU[:,0], color=params.colors.UpperCC, marker = '<', ls='--', label=f'Within monkey1')\n",
    "    ax.plot(x_, CCsU[:,1], marker = '<', ls='--', label=f'Within monkey2')\n",
    "    ax.plot(x_, CCsL, color=params.colors.LowerCC, marker = '>', ls=':', label=f'Control')\n",
    "\n",
    "\n",
    "    ax.set_ylim([-.05,1])\n",
    "    ax.set_xlim([.6,defs.n_components+.6])\n",
    "    ax.set_xlabel('Neural mode')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    # ax.set_title(f'{defs.areas[2]} Alignment')\n",
    "    ax.legend(loc=(.55,.67))\n",
    "    ax.set_ylabel('Canonical correlation')\n",
    "\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_bounds([1,defs.n_components])\n",
    "    ax.spines['left'].set_bounds([0,1])\n",
    "\n",
    "    info = '_'.join([ex_file[:-4], str(len(subset_target_groups)), str(defs.n_angle_groups), str(defs.min_trials_per_target), str(defs.match_mse_cutoff_perc)])\n",
    "    plt.savefig(\"figures/cca_permutated_\"+info+'.pdf', format= 'pdf')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Behavioural correlation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "@utility.report\n",
    "def plot_monkey_cca_corr(paired_dfs):\n",
    "\n",
    "    across_corrs_monkeys = defs.trim_across_monkey_corr(paired_dfs)\n",
    "    CC_corr_monkey=[]\n",
    "\n",
    "    # min_target_groups = np.inf\n",
    "    # for _,df1_,_ in paired_dfs:\n",
    "    #     n_groups = len(np.unique(df1.target_group))\n",
    "    #     min_target_groups = np.min(min_target_groups, n_groups)\n",
    "\n",
    "    #for each pair of sessions across monkeys\n",
    "    for i, (_,df1_,df2_) in enumerate(paired_dfs):\n",
    "        #get data for neural modes\n",
    "        AllData1_monkey,AllData2_monkey = defs.get_paired_data_arrays(df1_, df2_, defs.exec_epoch, area=defs.areas[2], model=defs.n_components)\n",
    "        # AllData2_monkey = defs.get_data_array(df2_, defs.exec_epoch, area=defs.areas[2], model=defs.n_components)\n",
    "        _,_, min_trials_monkey, min_time_monkey,_ = np.min((AllData1_monkey.shape,AllData2_monkey.shape),axis=0)\n",
    "\n",
    "        data1 = np.reshape(AllData1_monkey[:,:min_trials_monkey,:min_time_monkey,:], (-1,defs.n_components))\n",
    "        data2 = np.reshape(AllData2_monkey[:,:min_trials_monkey,:min_time_monkey,:], (-1,defs.n_components))\n",
    "\n",
    "        #get behavioral correlation\n",
    "        behav = np.array(across_corrs_monkeys[i])\n",
    "        # behav = behav[behav>params.Behav_corr_TH]\n",
    "\n",
    "        #perform CCA\n",
    "        CC_corr_monkey.append((dt.canoncorr(data1, data2)[:4].mean() , np.mean(behav)))\n",
    "    CC_corr_monkey = np.array(CC_corr_monkey)\n",
    "    \n",
    "    #plotting\n",
    "    fig, ax = plt.subplots()\n",
    "    ax.scatter(CC_corr_monkey[:,1],CC_corr_monkey[:,0], color=params.colors.MonkeyPts, label='Monkeys', zorder=0)\n",
    "    ax.set_xlabel('Behavioural correlation')\n",
    "    ax.set_xlabel('MSE')\n",
    "    ax.set_ylabel('Canonical correlation')\n",
    "    # ax.set_ylim([.53,.85])\n",
    "    # ax.spines['left'].set_bounds([.55,.85])\n",
    "    # ax.set_xlim([.69,.95])\n",
    "    # ax.spines['bottom'].set_bounds([.7,.95])\n",
    "    # ax.legend(loc=(0,.8))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.yaxis.set_major_formatter(FormatStrFormatter('$%0.2f$'))\n",
    "\n",
    "    return CC_corr_monkey[:,1], CC_corr_monkey[:,0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_monkey_cca_corr(paired_dfs);\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for ex_file, df1_, df2_ in paired_dfs[:2]:\n",
    "    #plot matched reaches\n",
    "    plt.figure()\n",
    "    ntrials = 10\n",
    "    colors = utility.get_colors(13, 'tab10')\n",
    "    alphas = np.linspace(0.2,1,defs.n_angle_groups)\n",
    "    # alphas = np.ones(defs.n_angle_groups)\n",
    "    target_groups = defs.target_groups\n",
    "    plt.figure()\n",
    "    df = df1_\n",
    "\n",
    "    subset_target_groups = np.unique(df1_.target_group)\n",
    "\n",
    "    center_ids = []\n",
    "    for tar in subset_target_groups[:]:\n",
    "        df1__ = df1_[df1_.target_group == tar]\n",
    "        df2__ = df2_[df2_.target_group == tar]\n",
    "\n",
    "        center_id = df2__.center_id.values[0]\n",
    "        angle_group = df2__.angle_group.values[0]\n",
    "        # if center_id in center_ids: continue #plot 1 target/center ID\n",
    "\n",
    "        center_ids.append(center_id)\n",
    "        center = df2__.center.values[0]\n",
    "        plt.scatter(center[0], center[1], s = 40, c= colors[center_id])\n",
    "\n",
    "        for i in range(1):\n",
    "            pos = df1__.pos_centered.values[i]\n",
    "            # pos = df1__.pos.values[i]\n",
    "            # targets = df1__.target_centered.values[i]\n",
    "\n",
    "            plt.plot(*pos[:].T, c=colors[center_id], label = 'Monkey C', alpha = alphas[angle_group])\n",
    "            # plt.scatter(*targets[:].T, s = 10, c=colors[center_id])\n",
    "\n",
    "            pos = df2__.pos_centered.values[i]\n",
    "            # pos = df2__.pos.values[i]\n",
    "            # targets = df2__.target_centered.values[i]\n",
    "            plt.plot(*pos[:].T, c=colors[center_id], linestyle = 'dotted', label = 'Monkey M', alpha = alphas[angle_group])\n",
    "            # plt.scatter(*targets[:].T, s = 10, c=colors[color_idx],marker = '^')\n",
    "    plt.gca().set_aspect(1)\n",
    "    plt.xlim([-10,10])\n",
    "    plt.ylim([-10,10])\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    by_label = dict(zip(labels, handles))\n",
    "    plt.legend(by_label.values(), by_label.keys())\n",
    "\n",
    "    info = '_'.join([ex_file[:-4], str(len(subset_target_groups)), str(defs.n_angle_groups), str(defs.min_trials_per_target), str(defs.match_mse_cutoff_perc)])\n",
    "    # plt.savefig(\"figures/paired_reaches_subset_targets\"+info+'.pdf', format= 'pdf')\n",
    "    plt.savefig(\"figures/paired_reaches_all_targets\"+info+'.pdf', format= 'pdf')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('cca')",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "f438ab41c639bccf4a0690d593feedbc6f327eadb4a78567565a57c35fedcdae"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
