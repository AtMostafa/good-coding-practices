{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "61b102c0",
   "metadata": {},
   "source": [
    "# part0: imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00059128",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:51:38.319503Z",
     "iopub.status.busy": "2022-09-02T21:51:38.319256Z",
     "iopub.status.idle": "2022-09-02T21:51:39.424976Z",
     "shell.execute_reply": "2022-09-02T21:51:39.424338Z"
    }
   },
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "\n",
    "import os, sys, pathlib\n",
    "from pprint import pprint\n",
    "from tqdm import tqdm\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import logging, warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "import PyPDF2 as ppdf\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.linalg as linalg\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.model_selection import cross_val_score, KFold\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "from scipy.stats import wilcoxon, mannwhitneyu\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib as mpl\n",
    "from matplotlib.ticker import MaxNLocator, FormatStrFormatter\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "import torch\n",
    "\n",
    "import pyaldata as pyal\n",
    "\n",
    "if \"__file__\" not in dir():\n",
    "    try:\n",
    "        NBPath = pathlib.Path.cwd()\n",
    "        RepoPath = NBPath.parent\n",
    "        os.chdir(RepoPath)\n",
    "\n",
    "        from tools import utilityTools as utility\n",
    "        from tools import dataTools as dt\n",
    "        from tools import lstm\n",
    "        import params\n",
    "        reload(params)\n",
    "        monkey_defs = params.monkey_defs\n",
    "        mouse_defs = params.mouse_defs\n",
    "\n",
    "        set_rc =  params.set_rc_params\n",
    "        set_rc()\n",
    "        root = params.root\n",
    "\n",
    "        os.chdir(RepoPath / 'monkey')\n",
    "        %run \"_dataset-selection.ipynb\"\n",
    "\n",
    "    finally:\n",
    "        os.chdir(NBPath)\n",
    "\n",
    "    %run \"fig3.ipynb\"\n",
    "\n",
    "    print('Done')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10aa2e5d-f25c-405a-ba73-2b2464c37b9e",
   "metadata": {},
   "source": [
    "# Part 1\n",
    "\n",
    "functions that plot each panel.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10159e45-a434-4695-b186-89f64ecf058c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "monkey population CCA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9fb936c-4ef3-4776-bd0f-bea0b5322a40",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:51:39.427724Z",
     "iopub.status.busy": "2022-09-02T21:51:39.427572Z",
     "iopub.status.idle": "2022-09-02T21:51:39.430698Z",
     "shell.execute_reply": "2022-09-02T21:51:39.430264Z"
    }
   },
   "outputs": [],
   "source": [
    "@utility.report\n",
    "def get_full_monkey_data():\n",
    "    full_list_MCx = []\n",
    "    for animal, sessionList in GoodDataList[monkey_defs.areas[2]].items():\n",
    "        if 'Mr' in animal:\n",
    "            continue  # to remove MrT\n",
    "        full_list_MCx.append((animal,sessionList))\n",
    "    full_list_MCx = [(animal,session) for animal,sessions in full_list_MCx for session in set(sessions)]\n",
    "    # load the DFs\n",
    "    allDFs_MCx = []\n",
    "    for animal, session in full_list_MCx:\n",
    "        path = root/animal/session\n",
    "        allDFs_MCx.append(monkey_defs.prep_general(dt.load_pyal_data(path)))\n",
    "\n",
    "    return full_list_MCx, allDFs_MCx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55ab4f90-108f-4c22-a744-3a35db5b2038",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:51:39.433155Z",
     "iopub.status.busy": "2022-09-02T21:51:39.432971Z",
     "iopub.status.idle": "2022-09-02T21:51:39.451509Z",
     "shell.execute_reply": "2022-09-02T21:51:39.451215Z"
    }
   },
   "outputs": [],
   "source": [
    "@utility.report\n",
    "def plot_monkey_cca_prep(ax, ax_hist, full_list_MCx, allDFs_MCx):\n",
    "    defs = monkey_defs\n",
    "    \n",
    "    pairFileList1 = []\n",
    "    for I, (animal1,session1) in enumerate(full_list_MCx):\n",
    "        for J, (animal2,session2) in enumerate(full_list_MCx):\n",
    "            if J<=I or animal1 == animal2: continue  # to repetitions\n",
    "            if 'Chewie' in animal1 and 'Chewie' in animal2: continue \n",
    "            pairFileList1.append((I,J))\n",
    "\n",
    "    side1df = [allDFs_MCx[i] for i,_ in pairFileList1]\n",
    "    side2df = [allDFs_MCx[j] for _,j in pairFileList1]\n",
    "    AllData1 = dt.get_data_array(side1df, monkey_defs.prep_epoch, area=monkey_defs.areas[2], model=monkey_defs.n_components)\n",
    "    AllData2 = dt.get_data_array(side2df, monkey_defs.prep_epoch, area=monkey_defs.areas[2], model=monkey_defs.n_components)\n",
    "    _,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "    allCCs=[]\n",
    "    for sessionData1,sessionData2 in zip(AllData1,AllData2):\n",
    "        data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "        data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "        allCCs.append(dt.canoncorr(data1, data2))\n",
    "    allCCs = np.array(allCCs).T\n",
    "\n",
    "    # lower bound\n",
    "    len_trial = int(np.round(np.diff(defs.WINDOW_exec)/defs.BIN_SIZE))\n",
    "    \n",
    "    single_FileList1 = []\n",
    "    for I, (animal1,session1) in enumerate(full_list_MCx):\n",
    "        for J, (animal2,session2) in enumerate(full_list_MCx):\n",
    "            if J<=I or animal1 == animal2: continue  # to repetitions\n",
    "            if 'Chewie' in animal1 and 'Chewie' in animal2: continue \n",
    "            single_FileList1.append((I,J))\n",
    "    n_iter = params.n_iter * 10\n",
    "    side1df = [allDFs_MCx[i] for i,_ in single_FileList1]\n",
    "    side2df = [allDFs_MCx[j] for _,j in single_FileList1]\n",
    "    AllData1 = defs._get_data_array(side1df, epoch_L=len_trial, area=defs.areas[2], model=defs.n_components)\n",
    "    AllData1_ = defs._get_data_array(side2df, epoch_L=len_trial, area=defs.areas[2], model=defs.n_components)\n",
    "    _,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData1_.shape),axis=0)\n",
    "\n",
    "    CCsL=[]\n",
    "    for sessionData1,sessionData2 in zip(AllData1,AllData1_):\n",
    "        r = []\n",
    "        for n in range(n_iter):\n",
    "            sessionData1_sh = params.rng.permutation(sessionData1,axis=0)\n",
    "            sessionData2_sh = params.rng.permutation(sessionData2,axis=0)\n",
    "\n",
    "            data1 = np.reshape(sessionData1_sh[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(sessionData2_sh[:,:min_trials,:min_time,:], (-1,defs.n_components))\n",
    "            r.append(dt.canoncorr(data1, data2))\n",
    "        CCsL.append(r)\n",
    "    CCsL = np.array(CCsL)\n",
    "    CCsL = np.percentile(CCsL, 1, axis=1).T\n",
    "\n",
    "    # Upper bound\n",
    "    single_FileList1 = []\n",
    "    for I, _ in enumerate(full_list_MCx):\n",
    "        single_FileList1.append(I)\n",
    "    side1df = [allDFs_MCx[i] for i in single_FileList1]\n",
    "    AllData1 = dt.get_data_array(side1df, monkey_defs.prep_epoch, area=monkey_defs.areas[2], model=monkey_defs.n_components)\n",
    "    n_shared_trial1 = AllData1.shape[2]\n",
    "    trialList1 = np.arange(n_shared_trial1)\n",
    "    CCsU=[]\n",
    "    for session, sessionData in enumerate(AllData1):\n",
    "        r = []\n",
    "        for n in range(n_iter):\n",
    "            params.rng.shuffle(trialList1)\n",
    "            # non-overlapping randomised trials\n",
    "            trial1 = trialList1[:n_shared_trial1//2]\n",
    "            trial2 = trialList1[-(n_shared_trial1//2):]\n",
    "            data1 = np.reshape(sessionData[:,trial1,:,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(sessionData[:,trial2,:,:], (-1,defs.n_components))\n",
    "            r.append(dt.canoncorr(data1, data2))\n",
    "        CCsU.append(r)\n",
    "    CCsU = np.array(CCsU)\n",
    "    CCsU = np.percentile(CCsU, 99, axis=1).T\n",
    "\n",
    "    # plotting\n",
    "    x_ = np.arange(1,monkey_defs.n_components+1)\n",
    "    utility.shaded_errorbar(ax, x_, allCCs, color=params.colors.MainCC, marker = 'o')\n",
    "    utility.shaded_errorbar(ax, x_, CCsU, color=params.colors.UpperCC, marker = '<', ls='--')\n",
    "    utility.shaded_errorbar(ax, x_, CCsL, color=params.colors.LowerCC, marker = '>', ls=':')\n",
    "\n",
    "    ax.set_ylim([-.05,1])\n",
    "    ax.set_xlim([.6,monkey_defs.n_components+.6])\n",
    "    ax.set_xlabel('Neural mode')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.set_ylabel('Canonical correlation')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_bounds([1,monkey_defs.n_components])\n",
    "    ax.spines['left'].set_bounds([0,1])\n",
    "    \n",
    "    #plot the hist\n",
    "    bins = np.arange(0,1,0.05)\n",
    "    ax_hist.xaxis.set_visible(False)\n",
    "    ax_hist.set_facecolor('None')\n",
    "    ax_hist.spines['bottom'].set_visible(False)\n",
    "    ax_hist.spines['right'].set_visible(False)\n",
    "    ax_hist.spines['top'].set_visible(False)\n",
    "    ax_hist.spines['left'].set_bounds([0,1])\n",
    "    ax_hist.set_ylim([-.05,1])\n",
    "    ax_hist.hist(allCCs[:4,:].mean(axis=0), bins=bins, density=True, label=f'Across $(n={allCCs.shape[1]}$)',\n",
    "            color=params.colors.MainCC, alpha=.8, orientation='horizontal')\n",
    "    ax_hist.hist(CCsU[:4,:].mean(axis=0), bins=bins, density=True,label=f'Within ($n={CCsU.shape[1]}$)',\n",
    "            color=params.colors.UpperCC, alpha=.8, orientation='horizontal')\n",
    "    ax_hist.hist(CCsL[:4,:].mean(axis=0), bins=bins, density=True, label=f'Control ($n={CCsL.shape[1]}$)',\n",
    "            color=params.colors.LowerCC, alpha=.8, orientation='horizontal')\n",
    "    \n",
    "    ax_hist.tick_params('y', direction='out')\n",
    "    ax_hist.set_yticklabels([])\n",
    "    ax_hist.legend(loc=(0,-.05))\n",
    "\n",
    "    #stats ###########################################################################\n",
    "    allCCs_median = np.median(allCCs[:4,:].mean(axis=0))\n",
    "    CCsU_median = np.median(CCsU[:4,:].mean(axis=0))\n",
    "    CCsL_median = np.median(CCsL[:4,:].mean(axis=0))\n",
    "    allCCs_mean = allCCs[:4,:].mean(axis=0)\n",
    "    CCsU_mean = CCsU[:4,:].mean(axis=0)\n",
    "    CCsL_mean = CCsL[:4,:].mean(axis=0)\n",
    "\n",
    "    #calc stats\n",
    "    ##for paired stats\n",
    "    side1CCsU_mean = [CCsU_mean[i] for i,_ in pairFileList1]\n",
    "    side2CCsU_mean = [CCsU_mean[j] for _,j in pairFileList1]\n",
    "    allCCsU_mean = side1CCsU_mean + side2CCsU_mean\n",
    "        \n",
    "    compare_upper_stats = wilcoxon(np.tile(allCCs_mean,2), allCCsU_mean)\n",
    "    compare_lower_stats = wilcoxon(allCCs_mean, CCsL_mean)\n",
    "\n",
    "    ##for unpaired stats\n",
    "    # compare_upper_stats = mannwhitneyu(allCCs_mean, CCsU_mean)\n",
    "    # compare_lower_stats = mannwhitneyu(allCCs_mean, CCsL_mean)\n",
    "\n",
    "    print(\"Across vs within:\", compare_upper_stats)\n",
    "    print(\"Across vs control:\", compare_lower_stats)\n",
    "\n",
    "    #annotate stats\n",
    "    xmin, xmax = ax_hist.get_xlim()\n",
    "    markerx = xmax+(xmax-xmin)*0.05\n",
    "    linex = xmax+(xmax-xmin)*0.15\n",
    "    textx = xmax+(xmax-xmin)*0.25\n",
    "    line_kwargs = dict(linewidth = 0.5, color = 'k')\n",
    "    text_kwargs = dict(ha='left', va='center')\n",
    "\n",
    "    ax_hist.scatter(markerx, allCCs_median, color = params.colors.MainCC, marker = '<')\n",
    "    ax_hist.scatter(markerx, CCsU_median, color = params.colors.UpperCC, marker = '<')\n",
    "    ax_hist.scatter(markerx, CCsL_median, color = params.colors.LowerCC, marker = '<')\n",
    "\n",
    "    ax_hist.plot([linex, linex], [allCCs_median, CCsU_median], **line_kwargs)\n",
    "    ax_hist.plot([linex, linex], [allCCs_median, CCsL_median], linestyle = '--', **line_kwargs)\n",
    "    \n",
    "    # print(dt.get_signif_annot(compare_upper_stats[1]))\n",
    "    ax_hist.text(textx, (allCCs_median + CCsU_median)/2, dt.get_signif_annot(compare_upper_stats[1]), **text_kwargs)\n",
    "    ax_hist.text(textx, (allCCs_median + CCsL_median)/2, dt.get_signif_annot(compare_lower_stats[1]), **text_kwargs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6d8e701-2e04-41c4-bc4c-2383ef0badd5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:51:39.453144Z",
     "iopub.status.busy": "2022-09-02T21:51:39.453045Z",
     "iopub.status.idle": "2022-09-02T21:51:58.754282Z",
     "shell.execute_reply": "2022-09-02T21:51:58.753949Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    plt.close('all')\n",
    "    set_rc()\n",
    "    fig=plt.figure(dpi=100)\n",
    "    gs = fig.add_gridspec(nrows=1, ncols=2, width_ratios=(5,1), wspace=0)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    ax_ = fig.add_subplot(gs[1])\n",
    "\n",
    "    \n",
    "    full_list_MCx, allDFs_MCx = get_full_monkey_data()\n",
    "    \n",
    "    plot_monkey_cca_prep(ax,ax_, full_list_MCx[10:13], allDFs_MCx)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4145a731-d518-4b69-bc46-78ca617b9745",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "decoding for monkeys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0659f95f-f4a6-4333-8480-9fa7594ea14a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:51:58.756018Z",
     "iopub.status.busy": "2022-09-02T21:51:58.755845Z",
     "iopub.status.idle": "2022-09-02T21:51:58.769828Z",
     "shell.execute_reply": "2022-09-02T21:51:58.769532Z"
    }
   },
   "outputs": [],
   "source": [
    "@utility.report\n",
    "def plot_monkey_target_decoding(ax, allDFs):\n",
    "    defs = monkey_defs\n",
    "    classifier_model = GaussianNB\n",
    "\n",
    "    within_score = {}\n",
    "    aligned_score = {}\n",
    "    unaligned_score = {}\n",
    "    for i, df1 in enumerate(tqdm(allDFs)):\n",
    "        if 'J' in df1.monkey[0]:\n",
    "            continue  # remove Jaco from this analysis\n",
    "        AllData = dt.get_data_array([df1], defs.prep_epoch, area=defs.areas[2], model=defs.n_components)\n",
    "        # adding history\n",
    "        AllData = dt.add_history_to_data_array(AllData,defs.MAX_HISTORY)\n",
    "\n",
    "        AllData1 = AllData[0,...]\n",
    "        _,n_trial,n_time,n_comp = AllData1.shape\n",
    "        # resizing\n",
    "        X1 = AllData1.reshape((-1,n_comp*n_time))\n",
    "        AllTar = np.repeat(np.arange(defs.n_targets),n_trial)\n",
    "        # train the decoder\n",
    "        _score=cross_val_score(classifier_model(),X1,AllTar,scoring='accuracy', cv=10).mean()\n",
    "        within_score[df1.session[0]] = _score\n",
    "\n",
    "        aligned_score[df1.session[0]] = {}\n",
    "        unaligned_score[df1.session[0]] = {}\n",
    "        for j, df2 in enumerate(tqdm(allDFs)):\n",
    "            if j <= i: continue\n",
    "            if df1.monkey[0] == df2.monkey[0]: continue\n",
    "            if 'Chewie' in df1.monkey[0] and 'Chewie' in df2.monkey[0]: continue\n",
    "            if 'J' in df1.monkey[0] or 'J' in df2.monkey[0]: continue  # remove Jaco from this analysis\n",
    "\n",
    "            AllData = dt.get_data_array([df1,df2], defs.prep_epoch, area=defs.areas[2], model=defs.n_components)\n",
    "\n",
    "            # adding history\n",
    "            AllData = dt.add_history_to_data_array(AllData, defs.MAX_HISTORY)\n",
    "\n",
    "            AllData1 = AllData[0,...] \n",
    "            AllData2 = AllData[1,...]\n",
    "            _,n_trial,n_time,n_comp = AllData1.shape\n",
    "\n",
    "            # resizing\n",
    "            X1 = AllData1.reshape((-1,n_comp))\n",
    "            X2 = AllData2.reshape((-1,n_comp))\n",
    "\n",
    "            *_,U,V = dt.canoncorr(X1, X2, fullReturn=True)\n",
    "            U = U.reshape((-1,n_comp*n_time))\n",
    "            V = V.reshape((-1,n_comp*n_time))\n",
    "            AllTar = np.repeat(np.arange(defs.n_targets),n_trial)\n",
    "            trial_index = np.arange(len(AllTar))\n",
    "            params.rng.shuffle(trial_index)\n",
    "            X_train, Y_train = U[trial_index,:], AllTar[trial_index]\n",
    "            params.rng.shuffle(trial_index)\n",
    "            X_test, Y_test   = V[trial_index,:], AllTar[trial_index]\n",
    "\n",
    "            # train the decoder\n",
    "            classifier = classifier_model()\n",
    "            classifier.fit(X_train, Y_train)\n",
    "            # test the decoder\n",
    "            _score = classifier.score(X_test,Y_test)\n",
    "            aligned_score[df1.session[0]][df2.session[0]]=_score\n",
    "\n",
    "            #Unaligned\n",
    "\n",
    "            X1 = X1.reshape((-1,n_comp*n_time))\n",
    "            X2 = X2.reshape((-1,n_comp*n_time))\n",
    "            AllTar = np.repeat(np.arange(defs.n_targets),n_trial)\n",
    "            trial_index = np.arange(len(AllTar))\n",
    "            params.rng.shuffle(trial_index)\n",
    "            X_train, Y_train = X1[trial_index,:], AllTar[trial_index]\n",
    "            params.rng.shuffle(trial_index)\n",
    "            X_test, Y_test   = X2[trial_index,:], AllTar[trial_index]\n",
    "            # train the decoder\n",
    "            classifier = classifier_model()\n",
    "            classifier.fit(X_train, Y_train)\n",
    "            # test the decoder\n",
    "            _score = classifier.score(X_test,Y_test)\n",
    "            unaligned_score[df1.session[0]][df2.session[0]]=_score\n",
    "\n",
    "    #======================== PLOTTING\n",
    "\n",
    "    pop_within = np.array(list(within_score.values()))\n",
    "    pop_aligned = np.array([val for key in aligned_score for val in aligned_score[key].values()])\n",
    "    pop_unaligned = np.array([val for key in unaligned_score for val in unaligned_score[key].values()])\n",
    "\n",
    "    ax.errorbar(1, pop_aligned.mean(), np.std(pop_aligned), label='Across\\n' r'(\\textit{aligned})',\n",
    "                color=params.colors.MainCC, fmt='-o', capsize=1.5)    \n",
    "    ax.errorbar(0, pop_unaligned.mean(), np.std(pop_unaligned), label='Across\\n' r'(\\textit{unaligned})',\n",
    "                color=params.colors.LowerCC, fmt='-o', capsize=1.5)\n",
    "    ax.errorbar(2, pop_within.mean(), np.std(pop_within), label='Within',\n",
    "                color=params.colors.UpperCC, fmt='-o', capsize=1.5)\n",
    "\n",
    "    unal_vals = []\n",
    "    al_vals = []\n",
    "    wi_vals = []\n",
    "    for file1, nested_dict in aligned_score.items():\n",
    "        wi_val1 = within_score[file1]\n",
    "        for file2, al_val in nested_dict.items():\n",
    "            wi_val2 = within_score[file2]\n",
    "            unal_val = unaligned_score[file1][file2]\n",
    "            ax.plot([0,1,2], [unal_val, al_val, wi_val1],\n",
    "                    color='gray', lw=.2, zorder=6, marker = 'o', ms=.1, alpha=.2)\n",
    "            ax.plot([1,2], [al_val, wi_val2],\n",
    "                    color='gray', lw=.2, zorder=6, marker = 'o', ms=.1, alpha=.2)\n",
    "\n",
    "            #for stats\n",
    "            unal_vals.append(unal_val)\n",
    "            al_vals.append(al_val)\n",
    "            wi_vals.append(wi_val1)\n",
    "            wi_vals.append(wi_val2)\n",
    "\n",
    "    ax.set_xlim([-0.2,2.2])\n",
    "    ax.set_xticks([0,1,2])\n",
    "    ax.set_xticklabels(['Across\\n' r'(\\textit{unaligned})',\n",
    "                        'Across\\n' r'(\\textit{aligned})',\n",
    "                        'Within'])\n",
    "    ax.set_ylabel('Classification accuracy')\n",
    "    ax.set_ylim([-.05,1])\n",
    "    # ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_bounds([0,2])\n",
    "    ax.spines['left'].set_bounds([0,1])\n",
    "\n",
    "    #stats ###########################################################################\n",
    "    #calc stats\n",
    "    ##for paired stats\n",
    "    compare_upper_stats = wilcoxon(np.repeat(al_vals,2), wi_vals)\n",
    "    compare_lower_stats = wilcoxon(al_vals, unal_vals)\n",
    "\n",
    "    print(\"Across vs within:\", compare_upper_stats)\n",
    "    print(\"Across vs control:\", compare_lower_stats)\n",
    "\n",
    "    #annotate stats\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    liney = ymax*0.95\n",
    "    texty = ymax*1\n",
    "    line_kwargs = dict(linewidth = 0.5, color = 'k')\n",
    "    text_kwargs = dict(ha='center', va='center')\n",
    "\n",
    "    ax.plot([0,1], [liney, liney], **line_kwargs)\n",
    "    ax.plot([1,2], [liney, liney], linestyle = '--', **line_kwargs)\n",
    "    \n",
    "    ax.text(0.5, texty, dt.get_signif_annot(compare_upper_stats[1]), **text_kwargs)\n",
    "    ax.text(1.5, texty, dt.get_signif_annot(compare_lower_stats[1]), **text_kwargs)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec8bea-0094-4e29-8328-efd20c1102c7",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:51:58.771305Z",
     "iopub.status.busy": "2022-09-02T21:51:58.771164Z",
     "iopub.status.idle": "2022-09-02T21:52:19.736822Z",
     "shell.execute_reply": "2022-09-02T21:52:19.736512Z"
    }
   },
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    plt.close('all')\n",
    "    set_rc()\n",
    "    fig=plt.figure(dpi=100)\n",
    "    ax = fig.add_subplot()\n",
    "\n",
    "    \n",
    "    full_list_MCx, allDFs_MCx = get_full_monkey_data()\n",
    "    \n",
    "    plot_monkey_target_decoding(ax, allDFs_MCx[10:15])\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6806511-6197-45b5-beb6-316a050c9c83",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "Mouse CCA plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22ceccd6-a508-4bef-84e7-9cffa87aed00",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:52:19.738501Z",
     "iopub.status.busy": "2022-09-02T21:52:19.738345Z",
     "iopub.status.idle": "2022-09-02T21:52:19.741564Z",
     "shell.execute_reply": "2022-09-02T21:52:19.741297Z"
    }
   },
   "outputs": [],
   "source": [
    "@utility.report\n",
    "def get_full_mouse_data():\n",
    "    defs = mouse_defs\n",
    "    \n",
    "    animalList = ['mouse-data']\n",
    "    animalFiles = []\n",
    "    for animal in animalList:\n",
    "        animalFiles.extend(utility.find_file(root / animal, 'mat'))\n",
    "\n",
    "    AllDFs=[]\n",
    "    for fname in animalFiles:\n",
    "        df = dt.load_pyal_data(fname)\n",
    "        df['mouse'] = fname.split(os.sep)[-1][fname.split(os.sep)[-1].find('WR'):].split('_')[0]\n",
    "        df['file'] = fname.split(os.sep)[-1]\n",
    "        df = defs.prep_general_mouse(df)\n",
    "        AllDFs.append(df)\n",
    "\n",
    "    allDFs_M1 = []\n",
    "    for df in AllDFs:\n",
    "        if 'M1_rates' in df.columns:\n",
    "            allDFs_M1.append(df)\n",
    "\n",
    "\n",
    "    allDFs_Str = []\n",
    "    for df in AllDFs:\n",
    "        if 'Str_rates' in df.columns:\n",
    "            allDFs_Str.append(df)\n",
    "            \n",
    "    return allDFs_M1, allDFs_Str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7231f59-8b48-4f87-a746-4a753c32730d",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:52:19.742952Z",
     "iopub.status.busy": "2022-09-02T21:52:19.742822Z",
     "iopub.status.idle": "2022-09-02T21:52:19.757212Z",
     "shell.execute_reply": "2022-09-02T21:52:19.756938Z"
    }
   },
   "outputs": [],
   "source": [
    "@utility.report\n",
    "def plot_mouse_str_cca(ax, ax_hist, allDFs_Str):\n",
    "    defs = mouse_defs\n",
    "\n",
    "    pairFileList2 = []\n",
    "    for I, df1 in enumerate(allDFs_Str):\n",
    "        for J, df2 in enumerate(allDFs_Str):\n",
    "            if J<=I or df1.mouse[0] == df2.mouse[0]: continue  # repetitions\n",
    "            pairFileList2.append((I,J))\n",
    "\n",
    "    side1df = [allDFs_Str[i] for i,_ in pairFileList2]\n",
    "    side2df = [allDFs_Str[j] for _,j in pairFileList2]\n",
    "    AllData1_ = dt.get_data_array(side1df, defs.exec_epoch, area=defs.areas[1], model=defs.n_components)\n",
    "    AllData2_ = dt.get_data_array(side2df, defs.exec_epoch, area=defs.areas[1], model=defs.n_components)\n",
    "    _,_, min_trials_, min_time_,_ = np.min((AllData1_.shape,AllData2_.shape),axis=0)\n",
    "    allCCs=[]\n",
    "    for sessionData1,sessionData2 in zip(AllData1_,AllData2_):\n",
    "        data1 = np.reshape(sessionData1[:,:min_trials_,:min_time_,:], (-1,defs.n_components))\n",
    "        data2 = np.reshape(sessionData2[:,:min_trials_,:min_time_,:], (-1,defs.n_components))\n",
    "        allCCs.append(dt.canoncorr(data1, data2))\n",
    "    allCCs = np.array(allCCs).T\n",
    "\n",
    "    # upper bound\n",
    "    AllData2 = dt.get_data_array(allDFs_Str, defs.exec_epoch, area=defs.areas[1], model=defs.n_components)\n",
    "    n_shared_trial2 = AllData2.shape[2]\n",
    "    trialList2 = np.arange(n_shared_trial2)\n",
    "    n_iter = params.n_iter * 10\n",
    "    CCsU=[]\n",
    "    for sessionData in AllData2:\n",
    "        r = []\n",
    "        for n in range(n_iter):\n",
    "            params.rng.shuffle(trialList2)\n",
    "            # non-overlapping randomised trials\n",
    "            trial1 = trialList2[:n_shared_trial2//2]\n",
    "            trial2 = trialList2[-(n_shared_trial2//2):]\n",
    "            data1 = np.reshape(sessionData[:,trial1,:,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(sessionData[:,trial2,:,:], (-1,defs.n_components))\n",
    "            r.append(dt.canoncorr(data1, data2))\n",
    "        CCsU.append(r)\n",
    "    CCsU = np.array(CCsU)\n",
    "    CCsU = np.percentile(CCsU, 99, axis=1).T\n",
    "\n",
    "    # lower bound\n",
    "    len_trial = int(np.round(np.diff(defs.WINDOW_exec)/defs.BIN_SIZE))\n",
    "    pairFileList2 = []\n",
    "    for I, df1 in enumerate(allDFs_Str):\n",
    "        for J, df2 in enumerate(allDFs_Str):\n",
    "            if J<=I or df1.mouse[0] == df2.mouse[0]: continue  # repetitions\n",
    "            pairFileList2.append((I,J))\n",
    "    side1df = [allDFs_Str[i] for i,_ in pairFileList2]\n",
    "    side2df = [allDFs_Str[j] for _,j in pairFileList2]\n",
    "    AllData1_ = dt.get_data_array(side1df, area=defs.areas[1], model=defs.n_components)\n",
    "    AllData2_ = dt.get_data_array(side2df, area=defs.areas[1], model=defs.n_components)\n",
    "    _,_, min_trials_, min_time_,_ = np.min((AllData1_.shape,AllData2_.shape),axis=0)\n",
    "\n",
    "    CCsL=[]\n",
    "    for sessionData1,sessionData2 in zip(AllData1_,AllData2_):\n",
    "        r = []\n",
    "        for n in range(n_iter):\n",
    "            sessionData1_sh = params.rng.permutation(sessionData1,axis=0)\n",
    "            sessionData2_sh = params.rng.permutation(sessionData2,axis=0)\n",
    "            time_idx = params.rng.integers(min_time_-len_trial)\n",
    "\n",
    "            data1 = np.reshape(sessionData1_sh[:,:min_trials_,time_idx:time_idx+len_trial,:], (-1,defs.n_components))\n",
    "            data2 = np.reshape(sessionData2_sh[:,:min_trials_,time_idx:time_idx+len_trial,:], (-1,defs.n_components))\n",
    "            r.append(dt.canoncorr(data1, data2))\n",
    "        CCsL.append(r)\n",
    "    CCsL = np.array(CCsL)\n",
    "    CCsL = np.percentile(CCsL, 1, axis=1).T\n",
    "\n",
    "\n",
    "    #====================================PLOTTING\n",
    "    \n",
    "    x_ = np.arange(1,defs.n_components+1)\n",
    "    utility.shaded_errorbar(ax, x_, allCCs, color=params.colors.MainCC, marker = 'o')\n",
    "    utility.shaded_errorbar(ax, x_, CCsU, color=params.colors.UpperCC, marker = '<', ls='--')\n",
    "    utility.shaded_errorbar(ax, x_, CCsL, color=params.colors.LowerCC, marker = '>', ls=':')\n",
    "\n",
    "    ax.set_ylim([-.05,1])\n",
    "    ax.set_xlim([.6,defs.n_components+.6])\n",
    "    ax.set_xlabel('Neural mode')\n",
    "    ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.set_ylabel('Canonical correlation')\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_bounds([1,defs.n_components])\n",
    "    ax.spines['left'].set_bounds([0,1])\n",
    "    \n",
    "    #plot the hist\n",
    "\n",
    "    bins = np.arange(0,1,0.05)\n",
    "    ax_hist.xaxis.set_visible(False)\n",
    "    ax_hist.set_facecolor('None')\n",
    "    ax_hist.spines['bottom'].set_visible(False)\n",
    "    ax_hist.spines['right'].set_visible(False)\n",
    "    ax_hist.spines['top'].set_visible(False)\n",
    "    ax_hist.spines['left'].set_bounds([0,1])\n",
    "    ax_hist.set_ylim([-.05,1])\n",
    "    ax_hist.hist(allCCs[:4,:].mean(axis=0), bins=bins, density=True, label=f'($n={allCCs.shape[1]}$)',\n",
    "            color=params.colors.MainCC, alpha=.8, orientation='horizontal')\n",
    "    ax_hist.hist(CCsU[:4,:].mean(axis=0), bins=bins, density=True,label=f'($n={CCsU.shape[1]}$)',\n",
    "            color=params.colors.UpperCC, alpha=.8, orientation='horizontal')\n",
    "    ax_hist.hist(CCsL[:4,:].mean(axis=0), bins=bins, density=True, label=f'($n={CCsL.shape[1]}$)',\n",
    "            color=params.colors.LowerCC, alpha=.8, orientation='horizontal')\n",
    "    \n",
    "    ax_hist.tick_params('y', direction='out')\n",
    "    ax_hist.set_yticklabels([])\n",
    "    ax_hist.legend(loc=(0,-.05))\n",
    "\n",
    "    #stats ###########################################################################\n",
    "    allCCs_median = np.median(allCCs[:4,:].mean(axis=0))\n",
    "    CCsU_median = np.median(CCsU[:4,:].mean(axis=0))\n",
    "    CCsL_median = np.median(CCsL[:4,:].mean(axis=0))\n",
    "    allCCs_mean = allCCs[:4,:].mean(axis=0)\n",
    "    CCsU_mean = CCsU[:4,:].mean(axis=0)\n",
    "    CCsL_mean = CCsL[:4,:].mean(axis=0)\n",
    "\n",
    "    #calc stats\n",
    "    ##for paired stats\n",
    "    side1CCsU_mean = [CCsU_mean[i] for i,_ in pairFileList2]\n",
    "    side2CCsU_mean = [CCsU_mean[j] for _,j in pairFileList2]\n",
    "    allCCsU_mean = side1CCsU_mean + side2CCsU_mean\n",
    "        \n",
    "    compare_upper_stats = wilcoxon(np.tile(allCCs_mean,2), allCCsU_mean)\n",
    "    compare_lower_stats = wilcoxon(allCCs_mean, CCsL_mean)\n",
    "\n",
    "    ##for unpaired stats\n",
    "    # compare_upper_stats = mannwhitneyu(allCCs_mean, CCsU_mean)\n",
    "    # compare_lower_stats = mannwhitneyu(allCCs_mean, CCsL_mean)\n",
    "\n",
    "    print(\"Across vs within:\", compare_upper_stats)\n",
    "    print(\"Across vs control:\", compare_lower_stats)\n",
    "\n",
    "    #annotate stats\n",
    "    xmin, xmax = ax_hist.get_xlim()\n",
    "    markerx = xmax+(xmax-xmin)*0.05\n",
    "    linex = xmax+(xmax-xmin)*0.15\n",
    "    textx = xmax+(xmax-xmin)*0.25\n",
    "    line_kwargs = dict(linewidth = 0.5, color = 'k')\n",
    "    text_kwargs = dict(ha='left', va='center')\n",
    "\n",
    "    ax_hist.scatter(markerx, allCCs_median, color = params.colors.MainCC, marker = '<')\n",
    "    ax_hist.scatter(markerx, CCsU_median, color = params.colors.UpperCC, marker = '<')\n",
    "    ax_hist.scatter(markerx, CCsL_median, color = params.colors.LowerCC, marker = '<')\n",
    "\n",
    "    ax_hist.plot([linex, linex], [allCCs_median, CCsU_median], **line_kwargs)\n",
    "    ax_hist.plot([linex, linex], [allCCs_median, CCsL_median], linestyle = '--', **line_kwargs)\n",
    "    \n",
    "    # print(dt.get_signif_annot(compare_upper_stats[1]))\n",
    "    ax_hist.text(textx, (allCCs_median + CCsU_median)/2, dt.get_signif_annot(compare_upper_stats[1]), **text_kwargs)\n",
    "    ax_hist.text(textx, (allCCs_median + CCsL_median)/2, dt.get_signif_annot(compare_lower_stats[1]), **text_kwargs)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6cc17a24-5dcc-43fe-9a37-0f6b11fa05a8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:52:19.758597Z",
     "iopub.status.busy": "2022-09-02T21:52:19.758394Z",
     "iopub.status.idle": "2022-09-02T21:52:30.080678Z",
     "shell.execute_reply": "2022-09-02T21:52:30.080310Z"
    }
   },
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    plt.close('all')\n",
    "    set_rc()\n",
    "    fig=plt.figure(dpi=100)\n",
    "    gs = fig.add_gridspec(nrows=1, ncols=2, width_ratios=(5,1), wspace=0)\n",
    "\n",
    "    ax = fig.add_subplot(gs[0])\n",
    "    ax_ = fig.add_subplot(gs[1])\n",
    "\n",
    "    \n",
    "    _, allDFs_Str = get_full_mouse_data()\n",
    "    \n",
    "    plot_mouse_str_cca(ax,ax_, allDFs_Str)\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ec52320-f43f-4c7b-8937-98bfb98075cd",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "STR decoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3e8ac2-8ac3-4dfa-8515-20dfe375707e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:52:30.082623Z",
     "iopub.status.busy": "2022-09-02T21:52:30.082458Z",
     "iopub.status.idle": "2022-09-02T21:52:30.171808Z",
     "shell.execute_reply": "2022-09-02T21:52:30.171520Z"
    }
   },
   "outputs": [],
   "source": [
    "@utility.report\n",
    "def plot_str_decoding(ax, AllDFs):\n",
    "    defs = mouse_defs\n",
    "    \n",
    "    #=========================\n",
    "    within_score = {}\n",
    "    aligned_score = {}\n",
    "    unaligned_score = {}\n",
    "    for i, df1 in enumerate(tqdm(AllDFs)):\n",
    "        animal1 = df1.mouse[0]\n",
    "\n",
    "        AllData, AllVel = defs.get_data_array_and_vel([df1], defs.exec_epoch_decode, area=defs.areas[1],\n",
    "                                                    n_components=defs.n_components)\n",
    "        # adding history\n",
    "        AllData = dt.add_history_to_data_array(AllData, defs.MAX_HISTORY)\n",
    "        AllData = AllData[...,defs.MAX_HISTORY:,:]\n",
    "        AllVel = AllVel[...,defs.MAX_HISTORY:,:]\n",
    "        *_,n_time,n_comp = AllData.shape\n",
    "        AllData1 = AllData[0,...]\n",
    "        AllVel1 = AllVel[0,...]\n",
    "        # resizing\n",
    "        X1 = AllData1.reshape((-1, n_time, n_comp))\n",
    "        AllVel1 = AllVel1.reshape((-1,n_time,3))\n",
    "\n",
    "        fold_score =[]\n",
    "        kf = KFold(n_splits=10)\n",
    "        for train_index, test_index in kf.split(X1[:,0,0]):\n",
    "            x_train, x_test = X1[train_index,...], X1[test_index,...]\n",
    "            y_train, y_test = AllVel1[train_index,...], AllVel1[test_index,...]\n",
    "\n",
    "            lstm_model = lstm.LSTMDecoder(input_dims=X1.shape[-1], output_dims=3)\n",
    "            lstm_model.fit(x_train=x_train, y_train=y_train, epochs = 10)\n",
    "            lstm_model.predict(x_test, y_test)\n",
    "            fold_score.append(lstm_model.score)\n",
    "        fold_score = np.median(fold_score)\n",
    "        within_score[df1.file[0]] = fold_score\n",
    "\n",
    "\n",
    "        aligned_score[df1.file[0]] = {}\n",
    "        unaligned_score[df1.file[0]] = {}\n",
    "        for j, df2 in enumerate(AllDFs):\n",
    "            if j < i: continue\n",
    "            animal2 = df2.mouse[0]\n",
    "            if animal1 == animal2: continue\n",
    "            \n",
    "            #================================\n",
    "            # Unaligned\n",
    "            AllData, AllVel = defs.get_data_array_and_vel([df1, df2], defs.exec_epoch_decode,\n",
    "                                                          area=defs.areas[1], n_components=defs.n_components)\n",
    "\n",
    "            # adding history\n",
    "            AllData = dt.add_history_to_data_array(AllData, defs.MAX_HISTORY)\n",
    "            AllData = AllData[...,defs.MAX_HISTORY:,:]\n",
    "            AllVel = AllVel[...,defs.MAX_HISTORY:,:]\n",
    "            AllData1 = AllData[0,...]\n",
    "            AllData2 = AllData[1,...]\n",
    "            AllVel1 = AllVel[0,...]\n",
    "            AllVel2 = AllVel[1,...]\n",
    "            # resizing\n",
    "            _,n_trial,n_time,n_comp = AllData1.shape\n",
    "            X1 = AllData1.reshape((-1,n_comp))\n",
    "            X2 = AllData2.reshape((-1,n_comp))\n",
    "            AllVel1 = AllVel1.reshape((-1,n_time,3))\n",
    "            AllVel2 = AllVel2.reshape((-1,n_time,3))\n",
    "\n",
    "            *_,U,V = dt.canoncorr(X1, X2, fullReturn=True)\n",
    "            U = U.reshape((-1,n_time,n_comp))\n",
    "            V = V.reshape((-1,n_time,n_comp))\n",
    "            X1 = X1.reshape((-1,n_time,n_comp))\n",
    "            X2 = X2.reshape((-1,n_time,n_comp))\n",
    "\n",
    "            lstm_model = lstm.LSTMDecoder(input_dims=U.shape[-1], output_dims=3)\n",
    "            lstm_model.fit(x_train=U, y_train=AllVel1)\n",
    "            lstm_model.predict(V, AllVel2)\n",
    "            aligned_score[df1.file[0]][df2.file[0]] = lstm_model.score.mean()\n",
    "\n",
    "            #================================\n",
    "            # Unaligned\n",
    "            lstm_model1 = lstm.LSTMDecoder(input_dims=X1.shape[-1], output_dims=3)\n",
    "            lstm_model1.fit(x_train=X1, y_train=AllVel1)\n",
    "            lstm_model1.predict(X2, AllVel2)\n",
    "            lstm_model2 = lstm.LSTMDecoder(input_dims=X1.shape[-1], output_dims=3)\n",
    "            lstm_model2.fit(x_train=X2, y_train=AllVel2)\n",
    "            lstm_model2.predict(X1, AllVel1)\n",
    "\n",
    "            unaligned_score[df1.file[0]][df2.file[0]] = (lstm_model1.score.mean() + lstm_model2.score.mean()) / 2\n",
    "\n",
    "\n",
    "    # return within_score, aligned_score, unaligned_score\n",
    "    #======================== PLOTTING\n",
    "    pop_within = np.array(list(within_score.values()))\n",
    "    pop_aligned = np.array([val for key in aligned_score for val in aligned_score[key].values()])\n",
    "    pop_unaligned = np.array([val for key in unaligned_score for val in unaligned_score[key].values()])\n",
    "\n",
    "    ax.errorbar(1, pop_aligned.mean(), np.std(pop_aligned), label='Across\\n' r'(\\textit{aligned})',\n",
    "                color=params.colors.MainCC, fmt='-o', capsize=1.5)    \n",
    "    ax.errorbar(0, pop_unaligned.mean(), np.std(pop_unaligned), label='Across\\n' r'(\\textit{unaligned})',\n",
    "                color=params.colors.LowerCC, fmt='-o', capsize=1.5)\n",
    "    ax.errorbar(2, pop_within.mean(), np.std(pop_within), label='Within',\n",
    "                color=params.colors.UpperCC, fmt='-o', capsize=1.5)\n",
    "\n",
    "    unal_vals = []\n",
    "    al_vals = []\n",
    "    wi_vals = []\n",
    "    for file1, nested_dict in aligned_score.items():\n",
    "        wi_val1 = within_score[file1]\n",
    "        for file2, al_val in nested_dict.items():\n",
    "            wi_val2 = within_score[file2]\n",
    "            unal_val = unaligned_score[file1][file2]\n",
    "            ax.plot([0,1,2], [unal_val, al_val, wi_val1],\n",
    "                    color='gray', lw=.2, zorder=6, marker = 'o', ms=.1, alpha=.2)\n",
    "            ax.plot([1,2], [al_val, wi_val2],\n",
    "                    color='gray', lw=.2, zorder=6, marker = 'o', ms=.1, alpha=.2)\n",
    "\n",
    "            #for stats\n",
    "            unal_vals.append(unal_val)\n",
    "            al_vals.append(al_val)\n",
    "            wi_vals.append(wi_val1)\n",
    "            wi_vals.append(wi_val2)\n",
    "            \n",
    "\n",
    "    ax.set_xlim([-0.2,2.2])\n",
    "    ax.set_xticks([0,1,2])\n",
    "    ax.set_xticklabels(['Across\\n' r'(\\textit{unaligned})',\n",
    "                        'Across\\n' r'(\\textit{aligned})',\n",
    "                        'Within'])\n",
    "    ax.set_ylabel('Prediction accuracy ($R^2$)')\n",
    "    ax.set_ylim([-.05,.8])\n",
    "    # ax.legend(loc='upper right', bbox_to_anchor=(1.1, 1))\n",
    "    ax.spines['top'].set_visible(False)\n",
    "    ax.spines['right'].set_visible(False)\n",
    "    ax.spines['bottom'].set_bounds([0,2])\n",
    "    ax.spines['left'].set_bounds([0,.8])\n",
    "\n",
    "    #stats ###########################################################################\n",
    "    #calc stats\n",
    "    ##for paired stats\n",
    "    compare_upper_stats = wilcoxon(np.repeat(al_vals,2), wi_vals)\n",
    "    compare_lower_stats = wilcoxon(al_vals, unal_vals)\n",
    "\n",
    "    print(\"Across vs within:\", compare_upper_stats)\n",
    "    print(\"Across vs control:\", compare_lower_stats)\n",
    "\n",
    "    #annotate stats\n",
    "    ymin, ymax = ax.get_ylim()\n",
    "    liney = ymax*0.95\n",
    "    texty = ymax*1\n",
    "    line_kwargs = dict(linewidth = 0.5, color = 'k')\n",
    "    text_kwargs = dict(ha='center', va='center')\n",
    "\n",
    "    ax.plot([0,1], [liney, liney], **line_kwargs)\n",
    "    ax.plot([1,2], [liney, liney], linestyle = '--', **line_kwargs)\n",
    "    \n",
    "    ax.text(0.5, texty, dt.get_signif_annot(compare_upper_stats[1]), **text_kwargs)\n",
    "    ax.text(1.5, texty, dt.get_signif_annot(compare_lower_stats[1]), **text_kwargs)\n",
    "\n",
    "\n",
    "    return within_score, aligned_score, unaligned_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa3229b8-09b8-4de5-a538-4e68e3bf47de",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:52:30.173407Z",
     "iopub.status.busy": "2022-09-02T21:52:30.173269Z",
     "iopub.status.idle": "2022-09-02T21:59:20.687449Z",
     "shell.execute_reply": "2022-09-02T21:59:20.687181Z"
    }
   },
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    plt.close('all')\n",
    "    set_rc()\n",
    "    fig=plt.figure(dpi=100)\n",
    "    ax = fig.add_subplot()\n",
    "    \n",
    "    _, allDFs_Str = get_full_mouse_data()\n",
    "    \n",
    "    within_score, aligned_score, unaligned_score = plot_str_decoding(ax, allDFs_Str)\n",
    "    plt.savefig('dummy.PDF')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3078eb21-79b2-4e9a-96be-f7cd141f9ed2",
   "metadata": {},
   "source": [
    "# Part 2\n",
    "Plot figure"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfde11e9-58cf-43f9-b95c-a132acb86d32",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-09-02T21:59:20.688930Z",
     "iopub.status.busy": "2022-09-02T21:59:20.688786Z",
     "iopub.status.idle": "2022-09-02T22:11:22.418233Z",
     "shell.execute_reply": "2022-09-02T22:11:22.417695Z"
    }
   },
   "outputs": [],
   "source": [
    "if \"__file__\" not in dir():\n",
    "    plt.close('all')\n",
    "    set_rc()\n",
    "    figsize= (params.LargeFig[0], params.panels.SmallH*2+.3)\n",
    "    fig=plt.figure(figsize=figsize)\n",
    "    \n",
    "   \n",
    "\n",
    "    ##########################################\n",
    "    # 1: PREP schematics\n",
    "    gs1   =utility.add_gridspec_abs(fig, nrows=1, ncols=1, top=figsize[1], \n",
    "                                    right=params.panels.schmatic[0],\n",
    "                                    width=params.panels.schmatic[0],\n",
    "                                    height=params.panels.cca_hist[1])\n",
    "    ax1   =fig.add_subplot(gs1[0])\n",
    "    ax1 = utility.phantom_axes(ax1)\n",
    "    ax1.set_facecolor([1,0,0,0])\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "    ##########################################\n",
    "    # 2: CCA plot for prep\n",
    "    gs2 = fig.add_gridspec(nrows=1, ncols=2,  width_ratios=(params.panels.cca[0],params.panels.cca_hist[0]-params.panels.cca[0]),\n",
    "                           left=gs1.right+(.3/figsize[0]),  # .7\" offset\n",
    "                           right=gs1.right+ (.3/figsize[0]) + params.panels.cca_hist[0]/figsize[0],\n",
    "                           bottom=gs1.bottom,\n",
    "                           top=gs1.top,\n",
    "                           wspace=0)\n",
    "\n",
    "    ax2 = fig.add_subplot(gs2[0])\n",
    "    ax2_ = fig.add_subplot(gs2[1])\n",
    "    \n",
    "    full_list_MCx, allDFs_MCx = get_full_monkey_data()\n",
    "    \n",
    "    plot_monkey_cca_prep(ax2,ax2_, full_list_MCx, allDFs_MCx)\n",
    "    \n",
    "\n",
    "    \n",
    "    \n",
    "    ##########################################\n",
    "    # 3: Decoding for prep\n",
    "    gs3   =utility.add_gridspec_abs(fig, nrows=1, ncols=1,\n",
    "                                    top=figsize[1],\n",
    "                                    right=figsize[0],\n",
    "                                    width=params.panels.decoding_hist[0],\n",
    "                                    height=params.panels.decoding_hist[1])\n",
    "\n",
    "    ax3 = fig.add_subplot(gs3[0])\n",
    "\n",
    "    \n",
    "    plot_monkey_target_decoding(ax3, allDFs_MCx)\n",
    "\n",
    "        \n",
    "    \n",
    "    ##########################################\n",
    "    # 4: schematics STR\n",
    "    gs4   =utility.add_gridspec_abs(fig, nrows=1, ncols=1, left=0, \n",
    "                                    bottom=gs1.bottom*figsize[1]-params.panels.schmatic[1],\n",
    "                                    width=params.panels.schmatic[0],\n",
    "                                    height=params.panels.cca_hist[1])\n",
    "    ax4   =fig.add_subplot(gs4[0])\n",
    "    ax4 = utility.phantom_axes(ax4)\n",
    "    ax4.set_facecolor([0,0,0,0])\n",
    "\n",
    "    \n",
    "    \n",
    "    \n",
    "    ##########################################\n",
    "    # 5: CCA plot for STR\n",
    "    gs5 = fig.add_gridspec(nrows=1, ncols=2,  width_ratios=(params.panels.cca[0],params.panels.cca_hist[0]-params.panels.cca[0]),\n",
    "                           left=gs2.left,\n",
    "                           right=gs2.right,\n",
    "                           bottom=gs4.bottom,\n",
    "                           top=gs4.top,\n",
    "                           wspace=0)\n",
    "\n",
    "    ax5 = fig.add_subplot(gs5[0])\n",
    "    ax5_ = fig.add_subplot(gs5[1])\n",
    "    \n",
    "    \n",
    "    _, allDFs_Str = get_full_mouse_data()\n",
    "    \n",
    "    plot_mouse_str_cca(ax5,ax5_, allDFs_Str)\n",
    "    \n",
    "\n",
    "    \n",
    "    #########################################\n",
    "    # 6: Decoding for STR\n",
    "    gs6   =utility.add_gridspec_abs(fig, nrows=1, ncols=1,\n",
    "                                    top=gs5.top*figsize[1],\n",
    "                                    right=figsize[0],\n",
    "                                    width=params.panels.decoding_hist[0],\n",
    "                                    height=params.panels.decoding_hist[1])\n",
    "\n",
    "    ax6 = fig.add_subplot(gs6[0])\n",
    "\n",
    "\n",
    "    within_score, aligned_score, unaligned_score = plot_str_decoding(ax6, allDFs_Str)\n",
    "\n",
    "\n",
    "    \n",
    "    \n",
    "#     fig.align_ylabels([ax1,ax4])\n",
    "    #############################################\n",
    "    #%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%%\n",
    "    AXES=(ax1,ax2,ax3,ax4,ax5,ax6)\n",
    "    OFFX=np.array([.02]*len(AXES))\n",
    "    OFFY=np.array([.03]*len(AXES))\n",
    "    # OFFX[[-1]]=0.12\n",
    "    # OFFX[[1]]=0.06\n",
    "    \n",
    "    params.add_panel_caption(axes=AXES, offsetX=OFFX, offsetY=OFFY)\n",
    "\n",
    "    fig.savefig(params.figPath / 'figure3.pdf', format='pdf', bbox_inches='tight')\n",
    "    \n",
    "    \n",
    "    ##########################################\n",
    "    # monkey schematics\n",
    " \n",
    "    thisPath  =str(params.figPath / 'figure3.pdf')\n",
    "    sketchPath=str(params.figPath / 'monkey-prep-schematic.pdf')\n",
    "    if os.path.exists(sketchPath):\n",
    "        f1=ppdf.PdfFileReader(thisPath).getPage(0)\n",
    "        f2=ppdf.PdfFileReader(sketchPath).getPage(0)\n",
    "\n",
    "        f1.mergeTranslatedPage(page2=f2, tx=10, ty=147, expand=False)\n",
    "\n",
    "        writer=ppdf.PdfFileWriter()\n",
    "        writer.addPage(f1)\n",
    "        with open(thisPath,'wb') as f3:\n",
    "            writer.write(f3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f2ce210",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "f438ab41c639bccf4a0690d593feedbc6f327eadb4a78567565a57c35fedcdae"
  },
  "kernelspec": {
   "display_name": "Python 3.10.5 ('cca': conda)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "146bff742f048d001dec7fafb91c8daeb769f9aeaad9c8d129a83c9f9fb9a9b2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
