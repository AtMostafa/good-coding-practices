{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d374a142",
   "metadata": {},
   "source": [
    "# part0: imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "05c18582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "from pprint import pprint\n",
    "import gc\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import logging, warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.linalg as linalg\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from tools import utilityTools as utility\n",
    "from tools import dataTools as dt\n",
    "import pyaldata as pyal\n",
    "\n",
    "%matplotlib inline\n",
    "reload(dt)\n",
    "\n",
    "# Global params\n",
    "root = pathlib.Path(\"/data\")\n",
    "\n",
    "BIN_SIZE = .03  # sec\n",
    "WINDOW_prep = (-.4, .05)  # sec\n",
    "WINDOW_exec = (-.05, .40)  # sec\n",
    "n_components = 10  # min between M1 and PMd\n",
    "areas = ('M1', 'PMd')\n",
    "\n",
    "prep_epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on',\n",
    "                                     rel_start=int(WINDOW_prep[0]/BIN_SIZE),\n",
    "                                     rel_end=int(WINDOW_prep[1]/BIN_SIZE)\n",
    "                                    )\n",
    "exec_epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on', \n",
    "                                     rel_start=int(WINDOW_exec[0]/BIN_SIZE),\n",
    "                                     rel_end=int(WINDOW_exec[1]/BIN_SIZE)\n",
    "                                    )\n",
    "fixation_epoch = pyal.generate_epoch_fun(start_point_name='idx_target_on', \n",
    "                                         rel_start=int(WINDOW_prep[0]/BIN_SIZE),\n",
    "                                         rel_end=int(WINDOW_prep[1]/BIN_SIZE)\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1079be8e",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "8197db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_id(trial):\n",
    "    return int(np.round((trial.target_direction + np.pi) / (0.25*np.pi))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77d24ba1-312e-4789-956a-764709bde983",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the variable `GoodDataList` contains the session names\n"
     ]
    }
   ],
   "source": [
    "def prep_general (df):\n",
    "    \"preprocessing general!\"\n",
    "    time_signals = [signal for signal in pyal.get_time_varying_fields(df) if 'spikes' in signal]\n",
    "    df[\"target_id\"] = df.apply(get_target_id, axis=1)  # add a field `target_id` with int values\n",
    "\n",
    "    for signal in time_signals:\n",
    "        df_ = pyal.remove_low_firing_neurons(df, signal, 1)\n",
    "    \n",
    "    df_= pyal.select_trials(df, df.result== 'R')\n",
    "    df_= pyal.select_trials(df_, df_.epoch=='BL')\n",
    "    \n",
    "    assert np.all(df_.bin_size == .01), 'bin size is not consistent!'\n",
    "    df_ = pyal.combine_time_bins(df_, int(BIN_SIZE/.01))\n",
    "    for signal in time_signals:\n",
    "        df_ = pyal.sqrt_transform_signal(df_, signal)\n",
    "        \n",
    "    df_= pyal.add_firing_rates(df_, 'smooth', std=0.05)\n",
    "    \n",
    "    \n",
    "    return df_\n",
    "\n",
    "\n",
    "%run dataset_selection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91b4fdec-3d7e-4b5e-b386-b67d2feb6685",
   "metadata": {},
   "source": [
    "# Decoding\n",
    "\n",
    "Decode the arm velocity from M1 activity.\n",
    "\n",
    "## idea\n",
    "\n",
    "train the decoder on monkey1 and decode the behaviour of monkey2\n",
    "\n",
    "### run a test decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f0c4ee13-a199-473e-bfff-ec426ccc3dd6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msafaie/REPOS/PyalData/pyaldata/tools.py:934: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n",
      "/home/msafaie/REPOS/PyalData/pyaldata/tools.py:934: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "fname = root / 'Chewie' / \"Chewie_CO_CS_2016-10-21.mat\"\n",
    "df = dt.load_pyal_data(fname)\n",
    "df = prep_general(df)\n",
    "df_ = pyal.restrict_to_interval(df, epoch_fun=exec_epoch)\n",
    "model = PCA(10).fit(pyal.concat_trials(df_, 'M1_rates'))\n",
    "df_ = pyal.apply_dim_reduce_model(df_, model, 'M1_rates', 'M1_pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "fccde7f0-0fa8-448b-83dd-7562d99470c6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_no_hist.shape=(4290, 10)\n"
     ]
    }
   ],
   "source": [
    "Y1, Y2 = pyal.concat_trials(df_, 'vel').T\n",
    "\n",
    "X_no_hist = pyal.concat_trials(df_, 'M1_pca')\n",
    "print(f'{X_no_hist.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "b0f04205-c510-4ca2-98ab-856e7e4bd4b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist = []\n",
    "for i in range(4):\n",
    "    _epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on', \n",
    "                                     rel_start=int(WINDOW_exec[0]/BIN_SIZE)-i,\n",
    "                                     rel_end=int(WINDOW_exec[1]/BIN_SIZE)-i\n",
    "                                    )\n",
    "    _df = pyal.restrict_to_interval(df, epoch_fun=_epoch)\n",
    "    _model = PCA(n_components).fit(pyal.concat_trials(_df, 'M1_rates'))\n",
    "    _df = pyal.apply_dim_reduce_model(_df, _model, 'M1_rates', '_pca')\n",
    "    X_hist.append(pyal.concat_trials(_df, '_pca'))\n",
    "\n",
    "X = np.concatenate ((*X_hist,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "700c9dbb-e678-44b5-b753-874bbc102bd5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.894084357732575"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X, Y1)\n",
    "reg.score(X, Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "206c41e6-7a2a-49e8-88e2-ccfd5093d921",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**across monkey decoding**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5fbe8326-d150-460c-96c9-5ffaa86e73e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_array_and_vel(data_list: list[pd.DataFrame], epoch , area: str ='M1', n_components: int =10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies PCA to the data and return a data matrix of the shape: sessions x targets x  trials x time x PCs\n",
    "    with the minimum number of trials and timepoints shared across all the datasets/targets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    `data_list`: list of pd.dataFrame datasets from pyal-data\n",
    "    `epoch`: an epoch function of the type `pyal.generate_epoch_fun`\n",
    "    `area`: area, either: 'M1', or 'S1', or 'PMd'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `AllData`: np.array\n",
    "\n",
    "    Signature\n",
    "    -------\n",
    "    AllData = get_data_array(data_list, execution_epoch, area='M1', n_components=10)\n",
    "    all_data = np.reshape(AllData, (-1,10))\n",
    "    \"\"\"\n",
    "    field = f'{area}_rates'\n",
    "    n_shared_trial = np.inf\n",
    "    for df in data_list:\n",
    "        for target in range(8):\n",
    "            df_ = pyal.select_trials(df, df.target_id== target)\n",
    "            n_shared_trial = np.min((df_.shape[0], n_shared_trial))\n",
    "\n",
    "    n_shared_trial = int(n_shared_trial)\n",
    "\n",
    "    # finding the number of timepoints\n",
    "    df_ = pyal.restrict_to_interval(df_,epoch_fun=epoch)\n",
    "    n_timepoints = int(df_[field][0].shape[0])\n",
    "\n",
    "    # pre-allocating the data matrix\n",
    "    AllData = np.empty((len(data_list), 8, n_shared_trial, n_timepoints, n_components))\n",
    "    AllVel  = np.empty((len(data_list), 8, n_shared_trial, n_timepoints, 2))\n",
    "    rng = np.random.default_rng(12345)\n",
    "    for session, df in enumerate(data_list):\n",
    "        df_ = pyal.restrict_to_interval(df, epoch_fun=epoch)\n",
    "        rates = np.concatenate(df_[field].values, axis=0)\n",
    "        rates -= np.mean(rates, axis=0)\n",
    "        rates_model = PCA(n_components=n_components, svd_solver='full').fit(rates)\n",
    "        df_ = pyal.apply_dim_reduce_model(df_, rates_model, field, '_pca');\n",
    "\n",
    "        for target in range(8):\n",
    "            df__ = pyal.select_trials(df_, df_.target_id==target)\n",
    "            all_id = df__.trial_id.to_numpy()\n",
    "            rng.shuffle(all_id)\n",
    "            # select the right number of trials to each target\n",
    "            df__ = pyal.select_trials(df__, lambda trial: trial.trial_id in all_id[:n_shared_trial])\n",
    "            for trial, (trial_rates,trial_vel) in enumerate(zip(df__._pca, df__.vel)):\n",
    "                AllData[session,target,trial, :, :] = trial_rates\n",
    "                AllVel[session,target,trial, :, :] = trial_vel\n",
    "    \n",
    "    return AllData, AllVel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d4267e92-49b6-4cf8-b726-12c9bb51dc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list = []\n",
    "for area in ('dualArea','M1'):\n",
    "    for animal, sessionList in GoodDataList[area].items():\n",
    "        if '2' in animal:\n",
    "            continue  # to remove Chewie2\n",
    "        full_list.extend(sessionList)\n",
    "\n",
    "full_list = tuple(set(full_list))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c6b673c7-21f1-4fdf-82d9-9ef216cbbdfa",
   "metadata": {},
   "source": [
    "select animals $A$ and $B$ to compute the the canonical axes, and then decode based on **different** sessions of both $A$ and $B$, i.e., sessions different from those used to compute the axes.\n",
    "\n",
    "Next level would be to decode other animals, $C$, $D$, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "0742aae0-7d5a-43b9-bab6-ea80b5bae845",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the DFs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "allDFs = []\n",
    "for session in full_list:\n",
    "    path = root/session.split('_')[0]/session\n",
    "    allDFs.append(prep_general(dt.load_pyal_data(path)))\n",
    "\n",
    "# pairIndexList = list[ tuple[(train1, train2, list[test])], ]\n",
    "pairIndexList = []\n",
    "for i, session1 in enumerate(full_list):\n",
    "    animal1 = session1.split('_')[0]\n",
    "    for j, session2 in enumerate(full_list):\n",
    "        animal2 = session2.split('_')[0]\n",
    "        if animal1 == animal2: continue\n",
    "        pairIndexList.append((i,j,[k for k, session3 in enumerate(full_list) if session3 not in session1+session2 and\n",
    "                                                                                session3.split('_')[0] in animal1+animal2 ]))\n",
    "\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b9eeda13-dd63-4df2-b2af-40b7d262de3e",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_HISTORY = 3  #int no of bins\n",
    "\n",
    "reg_objs = []\n",
    "# reg_scores = list[ tuple[train1, train2, list[tuple[test_x,test_y]]]]\n",
    "reg_scores = []\n",
    "for id1, id2, testList in pairIndexList:\n",
    "    AllData1, _ = get_data_array_and_vel([allDFs[id1]], exec_epoch, area=areas[0], n_components=n_components)\n",
    "    AllData2, AllVel2 = get_data_array_and_vel([allDFs[id2]], exec_epoch, area=areas[0], n_components=n_components)\n",
    "    # resizing\n",
    "    AllData1 = AllData1.reshape((-1,n_components))\n",
    "    AllData2 = AllData2.reshape((-1,n_components))\n",
    "    min_time = min([AllData1.shape[-2],AllData2.shape[-2]])\n",
    "    # adding history\n",
    "    X1_hist = []\n",
    "    X2_hist = []\n",
    "    for shift in range(MAX_HISTORY+1):\n",
    "        X1_hist.append(np.roll(AllData1, shift, axis=0))\n",
    "        X2_hist.append(np.roll(AllData2, shift, axis=0))\n",
    "    X1 = np.concatenate((*X1_hist,), axis=1)\n",
    "    X2 = np.concatenate((*X2_hist,), axis=1)\n",
    "    # controlling the size\n",
    "    AllVel2 = AllVel2.reshape((-1,2))[MAX_HISTORY:min_time,:]\n",
    "    X1 = X1[MAX_HISTORY:min_time,:]  # removing the leading zeros\n",
    "    X2 = X2[MAX_HISTORY:min_time,:]\n",
    "    \n",
    "    A,*_,U,_ = dt.canoncorr(X1, X2, fullReturn=True)\n",
    "    Y_x,Y_y = AllVel2.T\n",
    "    # train the decoder\n",
    "    reg_x = LinearRegression()\n",
    "    reg_y = LinearRegression()\n",
    "    reg_x.fit(U, Y_x)\n",
    "    reg_y.fit(U, Y_y)\n",
    "    reg_objs.append((reg_x,reg_y))\n",
    "    reg_scores.append((id1,id2,[]))\n",
    "    # test the decoding on unrelated sessions\n",
    "    for testId in testList:\n",
    "        AllDataTest, AllVelTest = get_data_array_and_vel([allDFs[testId]], exec_epoch, area=areas[0], n_components=n_components)\n",
    "        # resizing\n",
    "        AllDataTest = AllDataTest.reshape((-1,n_components))\n",
    "        # adding history\n",
    "        X_test_hist = []\n",
    "        for shift in range(MAX_HISTORY+1):\n",
    "            X_test_hist.append(np.roll(AllDataTest, shift, axis=0))\n",
    "        X_test = np.concatenate((*X_test_hist,), axis=1)\n",
    "        # controlling the size\n",
    "        AllVelTest = AllVelTest.reshape((-1,2))[MAX_HISTORY:,:]\n",
    "        X_test = X_test[MAX_HISTORY:,:]  # removing the leading zeros\n",
    "\n",
    "        Y_test_x,Y_test_y = AllVelTest.T\n",
    "        U_test = X_test @ A\n",
    "        # test the decoder\n",
    "        reg_scores[-1][2].append((reg_x.score(U_test, Y_test_x),\n",
    "                                  reg_y.score(U_test, Y_test_y)\n",
    "                                 )\n",
    "                                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f43d0823-39e6-41e6-b559-a007b31b7ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060fcfc4-9e91-4094-84ef-ed357615474f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "054aaa62-e43e-4c9f-87f9-25d7b13a316e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d58b44c4-441a-4999-bc01-5a4a052539d4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72da6499f934495e06c03d484049d4696c0f7b78c6b9c64cf8676e9ec2014a6a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
