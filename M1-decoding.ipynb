{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "06debf48",
   "metadata": {},
   "source": [
    "# part0: imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "f206a049",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "from pprint import pprint\n",
    "import gc\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import logging, warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.linalg as linalg\n",
    "import scipy.stats as stats\n",
    "from sklearn.linear_model import Ridge, LinearRegression\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.metrics import make_scorer, r2_score\n",
    "\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "from tools import utilityTools as utility\n",
    "from tools import dataTools as dt\n",
    "import pyaldata as pyal\n",
    "import set_rc_params as set_rc\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['png2x']\n",
    "reload(dt)\n",
    "reload(set_rc)\n",
    "\n",
    "# Global params\n",
    "set_rc.set_rc_params()\n",
    "root = pathlib.Path(\"/data\")\n",
    "\n",
    "BIN_SIZE = .03  # sec\n",
    "WINDOW_prep = (-.4, .05)  # sec\n",
    "WINDOW_exec = (-.05, .40)  # sec\n",
    "n_components = 10  # min between M1 and PMd\n",
    "areas = ('M1', 'PMd')\n",
    "MAX_HISTORY = 3  #int: no of bins to be added as history\n",
    "\n",
    "prep_epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on',\n",
    "                                     rel_start=int(WINDOW_prep[0]/BIN_SIZE),\n",
    "                                     rel_end=int(WINDOW_prep[1]/BIN_SIZE)\n",
    "                                    )\n",
    "exec_epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on', \n",
    "                                     rel_start=int(WINDOW_exec[0]/BIN_SIZE),\n",
    "                                     rel_end=int(WINDOW_exec[1]/BIN_SIZE)\n",
    "                                    )\n",
    "fixation_epoch = pyal.generate_epoch_fun(start_point_name='idx_target_on', \n",
    "                                         rel_start=int(WINDOW_prep[0]/BIN_SIZE),\n",
    "                                         rel_end=int(WINDOW_prep[1]/BIN_SIZE)\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52614bea",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "17fc7ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_id(trial):\n",
    "    return int(np.round((trial.target_direction + np.pi) / (0.25*np.pi))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "a56e627e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the variable `GoodDataList` contains the session names\n"
     ]
    }
   ],
   "source": [
    "def prep_general (df):\n",
    "    \"preprocessing general!\"\n",
    "    time_signals = [signal for signal in pyal.get_time_varying_fields(df) if 'spikes' in signal]\n",
    "    df[\"target_id\"] = df.apply(get_target_id, axis=1)  # add a field `target_id` with int values\n",
    "\n",
    "    for signal in time_signals:\n",
    "        df_ = pyal.remove_low_firing_neurons(df, signal, 1)\n",
    "    \n",
    "    df_= pyal.select_trials(df, df.result== 'R')\n",
    "    df_= pyal.select_trials(df_, df_.epoch=='BL')\n",
    "    \n",
    "    assert np.all(df_.bin_size == .01), 'bin size is not consistent!'\n",
    "    df_ = pyal.combine_time_bins(df_, int(BIN_SIZE/.01))\n",
    "    for signal in time_signals:\n",
    "        df_ = pyal.sqrt_transform_signal(df_, signal)\n",
    "        \n",
    "    df_= pyal.add_firing_rates(df_, 'smooth', std=0.05)\n",
    "    \n",
    "    \n",
    "    return df_\n",
    "\n",
    "\n",
    "%run dataset_selection.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "2bbdfd70-9fd5-4b43-a7dd-7ee092eb2c38",
   "metadata": {},
   "outputs": [],
   "source": [
    "def custom_r2_func(y_true, y_pred):\n",
    "    \"$R^2$ value as squared correlation coefficient, as per Gallego, NN 2020\"\n",
    "    return stats.pearsonr(y_true, y_pred)[0] ** 2\n",
    "\n",
    "custom_r2_scorer = make_scorer(custom_r2_func)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2df8b2",
   "metadata": {},
   "source": [
    "# Decoding\n",
    "\n",
    "Decode the arm velocity from M1 activity.\n",
    "\n",
    "## idea\n",
    "\n",
    "train the decoder on monkey1 and decode the behaviour of monkey2\n",
    "\n",
    ">**correction**: the idea is to decode kinematics from the aligned dynamics of 2 monkeys.\n",
    "\n",
    "### run a test decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "6dbdc35a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/msafaie/REPOS/PyalData/pyaldata/tools.py:934: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n",
      "/home/msafaie/REPOS/PyalData/pyaldata/tools.py:934: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "fname = root / 'Chewie' / \"Chewie_CO_CS_2016-10-21.mat\"\n",
    "df = dt.load_pyal_data(fname)\n",
    "df = prep_general(df)\n",
    "df_ = pyal.restrict_to_interval(df, epoch_fun=exec_epoch)\n",
    "model = PCA(10).fit(pyal.concat_trials(df_, 'M1_rates'))\n",
    "df_ = pyal.apply_dim_reduce_model(df_, model, 'M1_rates', 'M1_pca')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "655c4c4a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_no_hist.shape=(4290, 10)\n"
     ]
    }
   ],
   "source": [
    "Y1, Y2 = pyal.concat_trials(df_, 'vel').T\n",
    "\n",
    "X_no_hist = pyal.concat_trials(df_, 'M1_pca')\n",
    "print(f'{X_no_hist.shape=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "919fb867",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_hist = []\n",
    "for i in range(4):\n",
    "    _epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on', \n",
    "                                     rel_start=int(WINDOW_exec[0]/BIN_SIZE)-i,\n",
    "                                     rel_end=int(WINDOW_exec[1]/BIN_SIZE)-i\n",
    "                                    )\n",
    "    _df = pyal.restrict_to_interval(df, epoch_fun=_epoch)\n",
    "    _model = PCA(n_components).fit(pyal.concat_trials(_df, 'M1_rates'))\n",
    "    _df = pyal.apply_dim_reduce_model(_df, _model, 'M1_rates', '_pca')\n",
    "    X_hist.append(pyal.concat_trials(_df, '_pca'))\n",
    "\n",
    "X = np.concatenate ((*X_hist,), axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8a2f68ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "reg = LinearRegression()\n",
    "reg.fit(X, Y1)\n",
    "reg.score(X, Y1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec0f3d47",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "**getting the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49ed0ac3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_array_and_vel(data_list: list[pd.DataFrame], epoch , area: str ='M1', n_components: int =10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies PCA to the data and return a data matrix of the shape: sessions x targets x  trials x time x PCs\n",
    "    with the minimum number of trials and timepoints shared across all the datasets/targets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    `data_list`: list of pd.dataFrame datasets from pyal-data\n",
    "    `epoch`: an epoch function of the type `pyal.generate_epoch_fun`\n",
    "    `area`: area, either: 'M1', or 'S1', or 'PMd'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `AllData`: np.array\n",
    "\n",
    "    Signature\n",
    "    -------\n",
    "    AllData = get_data_array(data_list, execution_epoch, area='M1', n_components=10)\n",
    "    all_data = np.reshape(AllData, (-1,10))\n",
    "    \"\"\"\n",
    "    field = f'{area}_rates'\n",
    "    n_shared_trial = np.inf\n",
    "    for df in data_list:\n",
    "        for target in range(8):\n",
    "            df_ = pyal.select_trials(df, df.target_id== target)\n",
    "            n_shared_trial = np.min((df_.shape[0], n_shared_trial))\n",
    "\n",
    "    n_shared_trial = int(n_shared_trial)\n",
    "\n",
    "    # finding the number of timepoints\n",
    "    df_ = pyal.restrict_to_interval(df_,epoch_fun=epoch)\n",
    "    n_timepoints = int(df_[field][0].shape[0])\n",
    "\n",
    "    # pre-allocating the data matrix\n",
    "    AllData = np.empty((len(data_list), 8, n_shared_trial, n_timepoints, n_components))\n",
    "    AllVel  = np.empty((len(data_list), 8, n_shared_trial, n_timepoints, 2))\n",
    "    rng = np.random.default_rng(12345)\n",
    "    for session, df in enumerate(data_list):\n",
    "        df_ = pyal.restrict_to_interval(df, epoch_fun=epoch)\n",
    "        rates = np.concatenate(df_[field].values, axis=0)\n",
    "        rates -= np.mean(rates, axis=0)\n",
    "        rates_model = PCA(n_components=n_components, svd_solver='full').fit(rates)\n",
    "        df_ = pyal.apply_dim_reduce_model(df_, rates_model, field, '_pca');\n",
    "\n",
    "        for target in range(8):\n",
    "            df__ = pyal.select_trials(df_, df_.target_id==target)\n",
    "            all_id = df__.trial_id.to_numpy()\n",
    "            rng.shuffle(all_id)\n",
    "            # select the right number of trials to each target\n",
    "            df__ = pyal.select_trials(df__, lambda trial: trial.trial_id in all_id[:n_shared_trial])\n",
    "            for trial, (trial_rates,trial_vel) in enumerate(zip(df__._pca, df__.vel)):\n",
    "                AllData[session,target,trial, :, :] = trial_rates\n",
    "                AllVel[session,target,trial, :, :] = trial_vel\n",
    "    \n",
    "    return AllData, AllVel"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b135757",
   "metadata": {},
   "source": [
    "# Within animal decoding\n",
    "\n",
    "select sessions of the same animal $A$ and $B$ to compute the the canonical axes, and then decode kinematics. One decoder per pair of sessions, similar to the NN paper.\n",
    "\n",
    "decoder performance cross-validated on the second day in each pair"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c6aca7f0",
   "metadata": {},
   "outputs": [],
   "source": [
    "full_list = []\n",
    "for area in ('dualArea','M1'):\n",
    "    for animal, sessionList in GoodDataList[area].items():\n",
    "        if 'Mr' in animal:\n",
    "            continue  # to remove MrT\n",
    "        full_list.append((animal,sessionList))\n",
    "full_list = [(animal,session) for animal,sessions in full_list for session in set(sessions)]\n",
    "\n",
    "# load the DFs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "allDFs = []\n",
    "for animal, session in full_list:\n",
    "    path = root/animal/session\n",
    "    allDFs.append(prep_general(dt.load_pyal_data(path)))\n",
    "warnings.filterwarnings(\"default\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e9c789f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pairIndexList = list[ tuple[train1, list[test]]]\n",
    "pairIndexList = []\n",
    "for i, (animal1,session1) in enumerate(full_list):\n",
    "    pairIndexList.append((i,[]))\n",
    "    for j, (animal2,session2) in enumerate(full_list):\n",
    "        if animal1 != animal2: continue\n",
    "        if session1 == session2: continue\n",
    "        pairIndexList[-1][1].append(j)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "73267b99-bb3a-4420-a184-698064754086",
   "metadata": {},
   "source": [
    "cross-validating\n",
    "\n",
    "**The method below should not be used!! It is just for confirmiation of the next step**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fda4c7fa-b318-41e4-9278-e8635eb0961d",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_HISTORY = 3  #int: no of bins\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "reg_scores = []\n",
    "for id1, testList in pairIndexList:\n",
    "    for testId in testList:\n",
    "        AllData, AllVel = get_data_array_and_vel([allDFs[id1],allDFs[testId]], exec_epoch, area=areas[0], n_components=n_components)\n",
    "\n",
    "        # adding history\n",
    "        AllData = dt.add_history_to_data_array(AllData,MAX_HISTORY)\n",
    "\n",
    "        AllData1 = AllData[0,...]\n",
    "        AllData2 = AllData[1,...]\n",
    "        AllVel2 = AllVel[1,...]\n",
    "        # resizing\n",
    "        _,n_trial,n_time,n_comp = np.min(np.array((AllData1.shape,AllData2.shape)),axis=0)\n",
    "        X1 = AllData1.reshape((-1,n_comp))\n",
    "        X2 = AllData2.reshape((-1,n_comp))\n",
    "        AllVel2 = AllVel2.reshape((-1,2))\n",
    "    \n",
    "#         *_,U,V = dt.canoncorr(X1, X2, fullReturn=True)\n",
    "        Y_x,Y_y = AllVel2.T\n",
    "        # train the decoder\n",
    "        x_score = cross_val_score(LinearRegression(), X2, Y_x, cv=5, scoring = custom_r2_scorer)\n",
    "        y_score = cross_val_score(LinearRegression(), X2, Y_y, cv= 5, scoring = custom_r2_scorer)\n",
    "        reg_scores.append((id1,testId,(x_score.mean(),y_score.mean())))\n",
    "warnings.filterwarnings(\"default\")\n",
    "pop_score = []\n",
    "for _,_,scores in reg_scores:\n",
    "    pop_score.append(np.add(*scores)/2)\n",
    "pop_score = np.array(pop_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35300ec7-7863-4cc3-9b80-61a0b53b8640",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(pop_score.size)\n",
    "_,ax = plt.subplots()\n",
    "ax.plot(pop_score,'-o')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('Session pairs')\n",
    "ax.set_ylabel('Prediction accuracy ($R^2$)')\n",
    "ax.set_title('Velocity Decoding')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cecea1ff-8f1c-426f-82e8-55409c2e8c81",
   "metadata": {},
   "source": [
    "The above results are much lower than the NN paper. But the paper doesn't cross-validate, instead it trains on one day, tests on the other and reports the $R^2$ value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "29c1e5c6-4a7c-4455-854d-e7a602e615e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_HISTORY = 3  #int: no of bins\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "reg_scores = []\n",
    "animal_scores = {animal: [] for animal,_ in full_list}\n",
    "for id1, testList in pairIndexList:\n",
    "    for testId in testList:\n",
    "        AllData, AllVel = get_data_array_and_vel([allDFs[id1],allDFs[testId]], exec_epoch, area=areas[0], n_components=n_components)\n",
    "        \n",
    "        # adding history\n",
    "        AllData = dt.add_history_to_data_array(AllData,MAX_HISTORY)\n",
    "\n",
    "        AllData1 = AllData[0,...]\n",
    "        AllData2 = AllData[1,...]\n",
    "        AllVel1 = AllVel[0,...]\n",
    "        AllVel2 = AllVel[1,...]\n",
    "        # resizing\n",
    "        _,n_trial,n_time,n_comp = np.min(np.array((AllData1.shape,AllData2.shape)),axis=0)\n",
    "        X1 = AllData1.reshape((-1,n_comp))\n",
    "        X2 = AllData2.reshape((-1,n_comp))\n",
    "        # controlling the size\n",
    "        AllVel2 = AllVel2.reshape((-1,2))\n",
    "        AllVel1 = AllVel1.reshape((-1,2))\n",
    "    \n",
    "        *_,U,V = dt.canoncorr(X1, X2, fullReturn=True)\n",
    "        Y_train_x,Y_train_y = AllVel1.T\n",
    "        # train the decoder\n",
    "        reg_x = LinearRegression()\n",
    "        reg_y = LinearRegression()\n",
    "        reg_x.fit(U, Y_train_x)\n",
    "        reg_y.fit(U, Y_train_y)\n",
    "        # test the decoder\n",
    "        Y_test_x,Y_test_y = AllVel2.T\n",
    "        x_score = custom_r2_func(Y_test_x,reg_x.predict(V))\n",
    "        y_score = custom_r2_func(Y_test_y,reg_y.predict(V))\n",
    "        reg_scores.append((id1,testId,(x_score,y_score)))\n",
    "        animal_scores[full_list[id1][0]].append((x_score+y_score)/2)\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "pop_score2 = []\n",
    "for _,_,scores in reg_scores:\n",
    "    pop_score2.append(np.mean(scores))\n",
    "pop_score2 = np.array(pop_score2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bbd0dfb-79b2-4db9-b1fa-0ff069175ef3",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots()\n",
    "ax.plot(pop_score2,'-o')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('Session pairs')\n",
    "ax.set_ylabel('Prediction accuracy ($R^2$)')\n",
    "ax.set_title('Velocity Decoding --- same animal, across days')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34d82b50-b10a-4071-9ddc-18c1f80a1a3e",
   "metadata": {},
   "source": [
    "## Within animal, within session\n",
    "\n",
    "Same as *\"within-day\"* analysis of the NN paper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "36dae281-c9c5-4fde-87a9-e7bfa7bbce33",
   "metadata": {},
   "outputs": [],
   "source": [
    "MAX_HISTORY = 3  #int: no of bins\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "reg_scores = []\n",
    "animal_day_scores = {animal: [] for animal,_ in full_list}\n",
    "for i, df in enumerate(allDFs):\n",
    "    AllData, AllVel = get_data_array_and_vel([df], exec_epoch, area=areas[0], n_components=n_components)\n",
    "\n",
    "    # adding history\n",
    "    AllData = dt.add_history_to_data_array(AllData,MAX_HISTORY)\n",
    "   \n",
    "    n_trials = int(AllData.shape[-3] /2)\n",
    "    AllData1 = AllData[0,:,:n_trials, :,:]\n",
    "    AllData2 = AllData[0,:,-n_trials:,:,:]\n",
    "    AllVel1 = AllVel[0,:,:n_trials, :,:]\n",
    "    AllVel2 = AllVel[0,:,-n_trials:,:,:]\n",
    "    _,n_trial,n_time,n_comp = np.min(np.array((AllData1.shape,AllData2.shape)),axis=0)\n",
    "    # resizing\n",
    "    X1 = AllData1.reshape((-1,n_comp))\n",
    "    X2 = AllData2.reshape((-1,n_comp))\n",
    "    # controlling the size\n",
    "    AllVel2 = AllVel2.reshape((-1,2))\n",
    "    AllVel1 = AllVel1.reshape((-1,2))\n",
    "\n",
    "    *_,U,V = dt.canoncorr(X1, X2, fullReturn=True)\n",
    "    Y_train_x,Y_train_y = AllVel1.T\n",
    "    # train the decoder\n",
    "    reg_x = LinearRegression()\n",
    "    reg_y = LinearRegression()\n",
    "    reg_x.fit(U, Y_train_x)\n",
    "    reg_y.fit(U, Y_train_y)\n",
    "    # test the decoder\n",
    "    Y_test_x,Y_test_y = AllVel2.T\n",
    "    x_score = custom_r2_func(Y_test_x,reg_x.predict(V))\n",
    "    y_score = custom_r2_func(Y_test_y,reg_y.predict(V))\n",
    "    reg_scores.append((i,(x_score,y_score)))\n",
    "    animal_day_scores[full_list[i][0]].append((x_score+y_score)/2)\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "pop_score_day = []\n",
    "for _,scores in reg_scores:\n",
    "    pop_score_day.append(np.mean(scores))\n",
    "pop_score_day = np.array(pop_score_day)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abd45e03-e15a-4a5b-808b-849390eb03ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots()\n",
    "ax.plot(pop_score_day,'-o')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('Session pairs')\n",
    "ax.set_ylabel('Prediction accuracy ($R^2$)')\n",
    "ax.set_title('Velocity Decoding --- within day')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b8a4c64-1d26-430c-bf0c-ec6d7ac33894",
   "metadata": {
    "tags": []
   },
   "source": [
    "Now... Same as above, for different animals\n",
    "\n",
    "# Across animal decoding\n",
    "\n",
    "## With alignment "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "80f82835-b5b7-409f-be0f-adbd698fe1bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairIndex_across = []\n",
    "for i, (animal1,session1) in enumerate(full_list):\n",
    "    pairIndex_across.append((i,[]))\n",
    "    for j, (animal2,session2) in enumerate(full_list):\n",
    "        if animal1 == animal2: continue\n",
    "        if 'Chewie' in animal1 and 'Chewie' in animal2: continue\n",
    "        pairIndex_across[-1][1].append(j)\n",
    "pairIndex_across = [(i,j) for i,jList in pairIndex_across for j in jList]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8979a0d0-3725-4ae6-bab2-6db0cea5f661",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "reg_scores_across = []\n",
    "for id1, testId in pairIndex_across:\n",
    "    AllData, AllVel = get_data_array_and_vel([allDFs[id1],allDFs[testId]], exec_epoch, area=areas[0], n_components=n_components)\n",
    "    \n",
    "    # adding history\n",
    "    AllData = dt.add_history_to_data_array(AllData,MAX_HISTORY)\n",
    "\n",
    "    AllData1 = AllData[0,...]\n",
    "    AllData2 = AllData[1,...]\n",
    "    AllVel1 = AllVel[0,...]\n",
    "    AllVel2 = AllVel[1,...]\n",
    "    # resizing\n",
    "    _,n_trial,n_time,n_comp = np.min(np.array((AllData1.shape,AllData2.shape)),axis=0)\n",
    "\n",
    "    X1 = AllData1.reshape((-1,n_comp))\n",
    "    X2 = AllData2.reshape((-1,n_comp))\n",
    "    AllVel2 = AllVel2.reshape((-1,2))\n",
    "    AllVel1 = AllVel1.reshape((-1,2))\n",
    "\n",
    "    # train the decoder\n",
    "    *_,U,V = dt.canoncorr(X1, X2, fullReturn=True)\n",
    "    Y_train_x,Y_train_y = AllVel1.T\n",
    "    \n",
    "    reg_x, reg_y = LinearRegression(), LinearRegression()\n",
    "    reg_x.fit(U, Y_train_x)\n",
    "    reg_y.fit(U, Y_train_y)\n",
    "    # test the decoder\n",
    "    Y_test_x,Y_test_y = AllVel2.T\n",
    "    x_score = custom_r2_func(Y_test_x, reg_x.predict(V))\n",
    "    y_score = custom_r2_func(Y_test_y, reg_y.predict(V))\n",
    "    reg_scores_across.append((id1,testId,(x_score,y_score)))\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "pop_score_across = []\n",
    "for _,_,scores in reg_scores_across:\n",
    "    pop_score_across.append(np.mean(scores))\n",
    "pop_score_across = np.array(pop_score_across)\n",
    "\n",
    "normalised_across_score = []\n",
    "for idx1, idx2, scores in reg_scores_across:\n",
    "    across_score = np.add(*scores)/2\n",
    "    animal_avg = np.mean([*animal_scores[full_list[idx1][0]], *animal_scores[full_list[idx2][0]]])\n",
    "    normalised_across_score.append(across_score/animal_avg)\n",
    "normalised_across_score = np.array(normalised_across_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "460e6a97-f858-43e9-812f-9f0ceb478a69",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots()\n",
    "ax.plot(pop_score_across,'-o')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('Session pairs')\n",
    "ax.set_ylabel('Prediction accuracy ($R^2$)')\n",
    "ax.set_title('Velocity Decoding --- across animals')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "368c9eeb-baae-4f72-a004-0c175eab9997",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots()\n",
    "ax.plot(normalised_across_score,'-o')\n",
    "# ax.set_ylim([0,1])\n",
    "ax.set_xlabel('Session pairs')\n",
    "ax.set_ylabel(r'Normalised prediction accuracy')\n",
    "ax.set_title(r'Velocity Decoding: $\\frac{R^2_{across}}{max(R^2_{within})}$', pad = 15)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f612a187-120a-4a65-a71f-5affa76741b7",
   "metadata": {},
   "source": [
    "## Without alignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "641a32a9-be3c-4133-8a5f-a3370479ec6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "reg_latent_scores = []\n",
    "for id1, testId in pairIndex_across:\n",
    "    AllData, AllVel = get_data_array_and_vel([allDFs[id1],allDFs[testId]], exec_epoch, area=areas[0], n_components=n_components)\n",
    "\n",
    "    # adding history\n",
    "    AllData = dt.add_history_to_data_array(AllData,MAX_HISTORY)\n",
    "\n",
    "    AllData1 = AllData[0,...]\n",
    "    AllData2 = AllData[1,...]\n",
    "    AllVel1 = AllVel[0,...]\n",
    "    AllVel2 = AllVel[1,...]\n",
    "    # resizing\n",
    "    _,n_trial,n_time,n_comp = np.min(np.array((AllData1.shape,AllData2.shape)),axis=0)\n",
    "    X1 = AllData1.reshape((-1,n_comp))\n",
    "    X2 = AllData2.reshape((-1,n_comp))\n",
    "    AllVel2 = AllVel2.reshape((-1,2))\n",
    "    AllVel1 = AllVel1.reshape((-1,2))\n",
    "\n",
    "    # train the decoder\n",
    "#     *_,U,V = dt.canoncorr(X1, X2, fullReturn=True)\n",
    "    Y_train_x,Y_train_y = AllVel1.T\n",
    "    \n",
    "    reg_x, reg_y = LinearRegression(), LinearRegression()\n",
    "    reg_x.fit(X1, Y_train_x)\n",
    "    reg_y.fit(X1, Y_train_y)\n",
    "    # test the decoder\n",
    "    Y_test_x,Y_test_y = AllVel2.T\n",
    "    x_score = custom_r2_func(Y_test_x, reg_x.predict(X2))\n",
    "    y_score = custom_r2_func(Y_test_y, reg_y.predict(X2))\n",
    "    reg_latent_scores.append((id1,testId,(x_score,y_score)))\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "pop_latent_score = []\n",
    "for _,_,scores in reg_latent_scores:\n",
    "    pop_latent_score.append(np.mean(scores))\n",
    "pop_latent_score = np.array(pop_latent_score)\n",
    "\n",
    "normalised_latent_score = []\n",
    "for idx1, idx2, scores in reg_latent_scores:\n",
    "    across_score = np.add(*scores)/2\n",
    "    animal_avg = np.mean([*animal_scores[full_list[idx1][0]], *animal_scores[full_list[idx2][0]]])\n",
    "    normalised_latent_score.append(across_score/animal_avg)\n",
    "normalised_latent_score = np.array(normalised_latent_score)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "60bb3add-2369-4b4f-b59a-0656be65d9ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "_,ax = plt.subplots()\n",
    "ax.plot(pop_latent_score,'-o')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('Session pairs')\n",
    "ax.set_ylabel('Prediction accuracy ($R^2$)')\n",
    "ax.set_title('Velocity Decoding --- across animals without alignment')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ece9e1e2-6e5b-4af4-82f2-6ec24077b5ff",
   "metadata": {},
   "source": [
    "# Universal Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b55f67b7-6d94-49b1-a219-e5c25f351963",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"M1-universal-decoding.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e4c3fde-020a-4250-aa10-2f3d969a357b",
   "metadata": {},
   "source": [
    "# histograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "57585ed5-7bba-46f6-a157-8a2f4ad826ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots(dpi=100)\n",
    "\n",
    "bins = np.arange(0,1,0.05)\n",
    "ax.hist(pop_score_across, bins=bins, density=True, label=r'Across monkey (\\textit{aligned})', alpha=.8)\n",
    "# ax.hist(pop_score2, bins=bins, density=True, label='aligned across days', alpha=.8)\n",
    "ax.hist(pop_score_uni, bins=bins, density=True, label='Universal', alpha=.8)\n",
    "ax.hist(pop_score_day, bins=bins, density=True, label='Within monkey', alpha=.8)\n",
    "ax.hist(pop_latent_score, bins=bins, density=True, label=r'Across monkey (\\textit{unaligned})', alpha=.8)\n",
    "\n",
    "ax.set_title('Decoding velocity --- M1')\n",
    "ax.set_xlabel('Prediction accuracy ($R^2$)')\n",
    "ax.set_ylabel(r'Normalised \\# of sessions')\n",
    "ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.set_xlim([-.05,1])\n",
    "ax.legend(loc=(.2,.7))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72da6499f934495e06c03d484049d4696c0f7b78c6b9c64cf8676e9ec2014a6a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
