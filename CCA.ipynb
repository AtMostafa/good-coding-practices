{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "steady-diesel",
   "metadata": {},
   "source": [
    "# part0: imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "intellectual-consolidation",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "from pprint import pprint \n",
    "from importlib import reload\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from sklearn.decomposition import PCA\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "from tools import utilityTools as utility\n",
    "from tools import dataTools as dt\n",
    "import pyaldata as pyal\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "root= pathlib.Path(\"/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "scheduled-thermal",
   "metadata": {},
   "source": [
    "A second order wrapper for `dt.canoncorr` to make it link with `PyalData`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "outer-kinase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def CCA_pyal(df1:pd.DataFrame, field1: str, df2:pd.DataFrame =None, field2:str =None) -> np.array:\n",
    "    if df2 is None:\n",
    "        assert isinstance(field2,str), 'Enter a valid string in field2'\n",
    "        df2 = df1\n",
    "\n",
    "    d0 = np.concatenate(df1[field1].values, axis=0)\n",
    "    d1 = np.concatenate(df2[field2].values, axis=0)\n",
    "\n",
    "    # same number of timepoints in both matrices\n",
    "    n_samples = min ([d0.shape[0], d1.shape[0]])\n",
    "    d0 = d0[:n_samples,:]\n",
    "    d1 = d1[:n_samples,:]\n",
    "\n",
    "    CC = dt.canoncorr(d0, d1)\n",
    "\n",
    "    return CC"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "reserved-cargo",
   "metadata": {},
   "source": [
    "# Compare similar target in 2 animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ethical-saying",
   "metadata": {},
   "outputs": [],
   "source": [
    "animalList = ['Chewie', 'Mihili']\n",
    "\n",
    "animalFiles={}\n",
    "for animal in animalList:\n",
    "    animalFiles[animal] = utility.find_file(root / animal,'mat')\n",
    "\n",
    "# just keeping the CS sessions\n",
    "goodFiles = [file for animal in animalFiles for file in animalFiles[animal] if 'CS' in file]\n",
    "pprint(goodFiles)\n",
    "\n",
    "# keeping 1 session per animal\n",
    "fileList = ['/data/Chewie/Chewie_CO_CS_2016-10-14.mat', '/data/Mihili/Mihili_CO_VR_2014-03-03.mat']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "pleasant-partnership",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data=[]\n",
    "for fname in fileList:\n",
    "    df = pyal.mat2dataframe(fname, shift_idx_fields=True)\n",
    "    df_data.append(df)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "southern-conversation",
   "metadata": {},
   "source": [
    "all the preprocessing steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "contemporary-seller",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_pyalData (df):\n",
    "    df_= pyal.add_firing_rates(df, 'smooth')\n",
    "    df_= pyal.select_trials(df_, df_.result== 'R')\n",
    "    df_= pyal.select_trials(df_, df_.epoch=='BL')\n",
    "    df_= pyal.select_trials(df_, df_.target_direction < -2)\n",
    "    \n",
    "    df_ = pyal.remove_low_firing_neurons(df_, \"M1_spikes\", 1)\n",
    "    df_ = pyal.remove_low_firing_neurons(df_, \"PMd_spikes\", 1)\n",
    "    \n",
    "    df_ = pyal.restrict_to_interval(df_, start_point_name='idx_movement_on', rel_start=0, rel_end=30)\n",
    "    \n",
    "    M1_rates = np.concatenate(df_.M1_rates.values, axis=0)\n",
    "    pca_model = PCA(n_components=20, svd_solver='full');\n",
    "    pca_model.fit(M1_rates);\n",
    "    df_ = pyal.apply_dim_reduce_model(df_, pca_model, 'M1_rates', 'M1_pca');\n",
    "\n",
    "    PMd_rates = np.concatenate(df_.PMd_rates.values, axis=0)\n",
    "    pca_model = PCA(n_components=20, svd_solver='full');\n",
    "    pca_model.fit(PMd_rates);\n",
    "    df_ = pyal.apply_dim_reduce_model(df_, pca_model, 'PMd_rates', 'PMd_pca');\n",
    "\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "divine-government",
   "metadata": {},
   "source": [
    "Prep both sessions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fifth-nursery",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_data_ = [prep_pyalData(df) for  df in df_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "spare-dubai",
   "metadata": {},
   "source": [
    "plot the CCs across animals, to one target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "direct-double",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0_M1 = np.concatenate(df_data_[0].M1_pca.values, axis=0)\n",
    "d1_M1 = np.concatenate(df_data_[1].M1_pca.values, axis=0)\n",
    "\n",
    "# same number of timepoints in both matrices\n",
    "n_samples_M1 = min ([d0_M1.shape[0], d1_M1.shape[0]])\n",
    "d0_M1_ = d0_M1[:n_samples_M1,:]\n",
    "d1_M1_ = d1_M1[:n_samples_M1,:]\n",
    "\n",
    "CC_M1 = dt.canoncorr(d0_M1_, d1_M1_)\n",
    "\n",
    "_,ax = plt.subplots()\n",
    "\n",
    "ax.plot(CC_M1, 'r-o', label='aligned')\n",
    "\n",
    "ax.set_xlabel('components')\n",
    "ax.set_ylabel('correlation')\n",
    "ax.legend()\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_title('CCA between M1 of two monkeys');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "limited-rendering",
   "metadata": {},
   "source": [
    "#### BE WARE!\n",
    "\n",
    "results depend on the initial number of PCs, i.e., features in the data matrix.\n",
    "For 10 PCs, the max CC will be 0.75 instead of 0.85\n",
    "\n",
    "---\n",
    "Next, let's add to the plot above, within monkey M1 CCs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "northern-crawford",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0_PMd = np.concatenate(df_data_[0].PMd_pca.values, axis=0)\n",
    "d1_PMd = np.concatenate(df_data_[1].PMd_pca.values, axis=0)\n",
    "\n",
    "# same number of timepoints in both matrices\n",
    "n_samples_d0 = min ([d0_M1.shape[0], d0_PMd.shape[0]])\n",
    "n_samples_d1 = min ([d1_M1.shape[0], d1_PMd.shape[0]])\n",
    "d0_PMd_ = d0_PMd[:n_samples_d0,:]\n",
    "d1_PMd_ = d1_PMd[:n_samples_d1,:]\n",
    "n_samples_PMd = min ([d0_PMd.shape[0], d1_PMd.shape[0]])\n",
    "d0_PMd_ = d0_PMd[:n_samples_PMd,:]\n",
    "d1_PMd_ = d1_PMd[:n_samples_PMd,:]\n",
    "\n",
    "\n",
    "\n",
    "CC_d0 = dt.canoncorr(d0_M1_, d0_PMd_)\n",
    "CC_d1 = dt.canoncorr(d1_M1_, d1_PMd_)\n",
    "CC_PMd= dt.canoncorr(d0_PMd_, d1_PMd_)\n",
    "\n",
    "_,ax = plt.subplots()\n",
    "\n",
    "ax.plot(CC_M1, label='M1 across')\n",
    "ax.plot(CC_PMd, label='PMd across')\n",
    "ax.plot(CC_d0,'--', label='Monkey1 within')\n",
    "ax.plot(CC_d1,'--', label='Monkey2 within')\n",
    "\n",
    "ax.set_xlabel('components')\n",
    "ax.set_ylabel('canonical correlation')\n",
    "ax.legend()\n",
    "ax.set_ylim([0,1])\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.set_title('CCA --- to a single target', usetex=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "brazilian-surge",
   "metadata": {},
   "source": [
    "average over every target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "union-andrew",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_id(trial):\n",
    "    return int(np.round((trial.target_direction + np.pi) / (0.25*np.pi))) - 1\n",
    "\n",
    "def prep_pyalData_all_targets (df):\n",
    "    df[\"target_id\"] = df.apply(get_target_id, axis=1)  # add a field `target_id` with int values\n",
    "\n",
    "    df_= pyal.add_firing_rates(df, 'smooth')\n",
    "    df_= pyal.select_trials(df_, df_.result== 'R')\n",
    "    df_= pyal.select_trials(df_, df_.epoch=='BL')\n",
    "    \n",
    "    df_ = pyal.remove_low_firing_neurons(df_, \"M1_spikes\", 1)\n",
    "    df_ = pyal.remove_low_firing_neurons(df_, \"PMd_spikes\", 1)\n",
    "    \n",
    "    df_ = pyal.restrict_to_interval(df_, start_point_name='idx_movement_on', rel_start=0, rel_end=30)\n",
    "    \n",
    "    M1_rates = np.concatenate(df_.M1_rates.values, axis=0)\n",
    "    pca_model = PCA(n_components=20, svd_solver='full');\n",
    "    pca_model.fit(M1_rates);\n",
    "    df_ = pyal.apply_dim_reduce_model(df_, pca_model, 'M1_rates', 'M1_pca');\n",
    "\n",
    "    PMd_rates = np.concatenate(df_.PMd_rates.values, axis=0)\n",
    "    pca_model = PCA(n_components=20, svd_solver='full');\n",
    "    pca_model.fit(PMd_rates);\n",
    "    df_ = pyal.apply_dim_reduce_model(df_, pca_model, 'PMd_rates', 'PMd_pca');\n",
    "\n",
    "    \n",
    "    return df_\n",
    "\n",
    "\n",
    "df_data__ = [prep_pyalData_all_targets(df) for  df in df_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "governing-nickname",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "CC=[]\n",
    "for target in range(8):\n",
    "    dfs = [pyal.select_trials(df_,df_.target_id==target) for df_ in df_data__]\n",
    "    \n",
    "    # monkey0: m1~pmd\n",
    "    CC.append(CCA_pyal(dfs[0],'M1_pca', dfs[0],'PMd_pca'))\n",
    "    # monkey1: m1~pmd\n",
    "    CC.append(CCA_pyal(dfs[1],'M1_pca', dfs[1],'PMd_pca'))\n",
    "    # m1: mon1~mon2\n",
    "    CC.append(CCA_pyal(dfs[0],'M1_pca', dfs[1],'M1_pca'))\n",
    "    # pmd: mon1~mon2\n",
    "    CC.append(CCA_pyal(dfs[0],'PMd_pca', dfs[1],'PMd_pca'))\n",
    "\n",
    "CC= np.array(CC)\n",
    "\n",
    "ax.plot(np.mean(CC[0::8,:],axis=0),label='monkey0: m1~pmd')\n",
    "ax.plot(np.mean(CC[1::8,:],axis=0),label='monkey1: m1~pmd')\n",
    "ax.plot(np.mean(CC[2::8,:],axis=0),label='m1: mon1~mon2')\n",
    "ax.plot(np.mean(CC[3::8,:],axis=0),label='pmd: mon1~mon2')\n",
    "\n",
    "ax.set_xlabel('components')\n",
    "ax.set_ylabel('canonical correlation')\n",
    "ax.legend()\n",
    "ax.set_ylim([0,1])\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.set_title('CCA --- average of all targets', usetex=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "distinct-blink",
   "metadata": {},
   "source": [
    "let's also add inter target CCA in individual animals"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "capital-brief",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,ax = plt.subplots()\n",
    "\n",
    "target_CC=[]\n",
    "for target0 in range(8):\n",
    "    dfs0 = [pyal.select_trials(df_,df_.target_id==target0) for df_ in df_data__]\n",
    "    for target1 in range(target0,8):\n",
    "        dfs1 = [pyal.select_trials(df_,df_.target_id==target1) for df_ in df_data__]\n",
    "        \n",
    "        target_CC.append(CCA_pyal(dfs0[0],'M1_pca', dfs1[0],'M1_pca'))\n",
    "        target_CC.append(CCA_pyal(dfs0[1],'M1_pca', dfs1[1],'M1_pca'))\n",
    "\n",
    "target_CC= np.array(target_CC)\n",
    "\n",
    "\n",
    "ax.plot(np.mean(CC[0::8,:],axis=0),label='monkey0: m1~pmd')\n",
    "ax.plot(np.mean(CC[1::8,:],axis=0),label='monkey1: m1~pmd')\n",
    "ax.plot(np.mean(CC[2::8,:],axis=0),label='m1: mon1~mon2')\n",
    "ax.plot(np.mean(CC[3::8,:],axis=0),label='pmd: mon1~mon2')\n",
    "\n",
    "ax.plot(np.mean(target_CC,axis=0),'o-',label='m1: inter-target')\n",
    "\n",
    "ax.set_xlabel('components')\n",
    "ax.set_ylabel('canonical correlation')\n",
    "ax.legend()\n",
    "ax.set_ylim([0,1])\n",
    "ax.xaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "ax.set_title('CCA --- average of all targets', usetex=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "grave-omega",
   "metadata": {},
   "source": [
    "## Ideas:\n",
    "+ **Explained variance**:\n",
    ">it'd be interesting to look at the variance explained by the shared dimensions though (both within monkey and across).\n",
    "\n",
    "Calculating the explained variance could be through the method outlined in the paper: \"Cortical population activity within a preserved neural manifold underlies multiple motor behaviors\"\n",
    "\n",
    "+ **Fewer PCs**: So far, CCA has converged in every case.\n",
    "Might be due to 20 PCs as input which gives CCA all the information it can ask for!\n",
    "\n",
    "+ **stability**: use the same transformations for aligning later sessions, hoping M1-PMd drops in accuracy and M1-M1 across animals, doesn't?!\n",
    "\n",
    "+ **non-movement epochs**: If the idea is that CCA reveals the underlying structure, CC axes should generalise across different time point in a trial;\n",
    "i.e., post stimulus and post movement, ...\n",
    "\n",
    "\n",
    "---\n",
    "Now, let's decode animals behaviour based on other animal's activity"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
