{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6dbda86d",
   "metadata": {},
   "source": [
    "# part0: imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2b7a810",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "from pprint import pprint\n",
    "import gc\n",
    "import pickle\n",
    "from importlib import reload\n",
    "import logging, warnings\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "from IPython.display import display, clear_output\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import sklearn\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.linalg as linalg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "from matplotlib.collections import LineCollection\n",
    "\n",
    "from tools import utilityTools as utility\n",
    "from tools import dataTools as dt\n",
    "import pyaldata as pyal\n",
    "import set_rc_params as set_rc\n",
    "\n",
    "\n",
    "%matplotlib inline\n",
    "%config InlineBackend.figure_formats = ['png2x']\n",
    "reload(dt)\n",
    "reload(set_rc)\n",
    "\n",
    "# Global params\n",
    "set_rc.set_rc_params()\n",
    "root = pathlib.Path(\"/data\")\n",
    "\n",
    "BIN_SIZE = .03  # sec\n",
    "WINDOW_ctrl = (-.95, -.5)\n",
    "WINDOW_prep = (-.4, .05)  # sec\n",
    "WINDOW_exec = (-.05, .40)  # sec\n",
    "n_components = 10  # min between M1 and PMd\n",
    "areas = ('M1', 'Str')\n",
    "\n",
    "prep_epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on',\n",
    "                                     rel_start=int(WINDOW_prep[0]/BIN_SIZE),\n",
    "                                     rel_end=int(WINDOW_prep[1]/BIN_SIZE)\n",
    "                                    )\n",
    "\n",
    "exec_epoch = pyal.generate_epoch_fun(start_point_name='idx_pull_off',\n",
    "                                     rel_start=int(-WINDOW_exec[1]/BIN_SIZE),\n",
    "                                     rel_end=int(-WINDOW_exec[0]/BIN_SIZE)\n",
    "                                    )\n",
    "\n",
    "fixation_epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on', \n",
    "                                         rel_start=int(WINDOW_ctrl[0]/BIN_SIZE),\n",
    "                                         rel_end=int(WINDOW_ctrl[1]/BIN_SIZE)\n",
    "                                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b4cce64",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_general_mouse (df):\n",
    "    \"preprocessing general! for J. Dudman mouse data\"\n",
    "    # rename unit fields\n",
    "    old_fields = [col for col in df.columns.values if 'unit' in col]\n",
    "    new_fields = ['M1_spikes' if 'Ctx' in col else 'Str_spikes' for col in old_fields]\n",
    "    df_ = df.rename(columns = {old:new for old,new in zip(old_fields,new_fields)})\n",
    "    # change spikes datatype\n",
    "    for signal in new_fields:\n",
    "        df_[signal] = [np.nan_to_num(x=s.toarray().T, nan=0) for s in df_[signal]]\n",
    "    # add trial_id\n",
    "    df_['trial_id'] = np.arange(1,df_.shape[0]+1)\n",
    "    # only keep good trials\n",
    "    df_= pyal.select_trials(df_, df_.trialType== 'sp')\n",
    "    # fill no-laser trials (and index fields) with zero\n",
    "    n_bins = df_[new_fields[0]][0].shape[0]\n",
    "    var_len_fields = [ 'spkPullIdx', 'spkRchIdx', 'spkTimeBlaserI']\n",
    "    fill_zeros = lambda a: a if len(a)>1 else np.zeros((n_bins,))\n",
    "    for field in var_len_fields:\n",
    "        if field not in df_.columns:continue\n",
    "        df_[field] = [fill_zeros(s) for s in df_[field]]\n",
    "    # fill fields that are cut with np.nans and remove trials that are too long or don't exist\n",
    "    cut_fields = ['hTrjB', 'hVelB']\n",
    "    df_['badIndex'] = [max(trialT.shape)>n_bins or\n",
    "                       max(trialV.shape)>n_bins or \n",
    "                       max(trialT.shape) < 2 or \n",
    "                       max(trialV.shape) < 2 or \n",
    "                       np.isnan(trialT).sum() > 5 for trialT,trialV in zip(df_.hTrjB,df_.hVelB)]\n",
    "    df_= pyal.select_trials(df_, df_.badIndex == False)\n",
    "    df_.drop('badIndex', axis=1, inplace=True)\n",
    "    fill_nans = lambda a: a if max(a.shape)==n_bins else np.pad(a, ((0,n_bins-max(a.shape)),(0,0)), 'constant', constant_values=(np.nan,))\n",
    "    for field in cut_fields:\n",
    "        if field not in df_.columns:continue\n",
    "        df_[field] = [fill_nans(s.T) for s in df_[field]]   \n",
    "    # add bin_size\n",
    "    df_['bin_size']=0.01  # data has 10ms bin size\n",
    "    # add idx_movement_on which is exactly at t=df.timeAlign\n",
    "    df_['idx_movement_on'] = [np.argmin(np.abs(s-i)) for i,s in zip(df_['timeAlign'],df_['spkTimeBins'])]\n",
    "    # add pull start idx\n",
    "    df_['idx_pull_on'] = [pullIdx.nonzero()[0][0] if len(pullIdx.nonzero()[0])>0 else np.nan for pullIdx in df_.spkPullIdx]\n",
    "    # add pull stop idx\n",
    "    df_['idx_pull_off'] = [pull.nonzero()[0][-1] for pull in df_.spkPullIdx]\n",
    "    # remove trials with no pull idx\n",
    "    df_.dropna(subset=['idx_pull_on'], inplace=True)\n",
    "    df_.idx_pull_on = df_.idx_pull_on.astype(np.int32)\n",
    "    df_.index = np.arange(df_.shape[0])\n",
    "    # add target_id\n",
    "    df_['target_id'] = np.remainder(df_['blNumber'].to_numpy(), 4)\n",
    "\n",
    "    for signal in new_fields:\n",
    "        df_ = pyal.remove_low_firing_neurons(df_, signal, 1)\n",
    "    \n",
    "    df_= pyal.select_trials(df_, df_.trialType== 'sp')\n",
    "    try:\n",
    "        noLaserIndex = [i for i,laserData in enumerate(df_.spkTimeBlaserI) if not np.any(laserData)]\n",
    "        df_= pyal.select_trials(df_, noLaserIndex)\n",
    "    except AttributeError:\n",
    "        # due to absence of this field in no-laser sessions\n",
    "        pass\n",
    "    \n",
    "    df_ = pyal.combine_time_bins(df_, int(BIN_SIZE/.01))\n",
    "    for signal in new_fields:\n",
    "        df_ = pyal.sqrt_transform_signal(df_, signal)\n",
    "        \n",
    "    df_= pyal.add_firing_rates(df_, 'smooth', std=0.05)\n",
    "    \n",
    "    return df_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72859ce7",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Load the lower bound values for the canonical correlations\n",
    "\n",
    "takes a couple of minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05da56d3",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"_mouse-lower-bound.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5802ab1c",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "#### Load the upper bound values for the canonical correlations\n",
    "\n",
    "takes a couple of minutes to execute."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d00eb12",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run \"_mouse-upper-bound.ipynb\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e896ba5",
   "metadata": {
    "tags": []
   },
   "source": [
    "# reading data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4d1d774",
   "metadata": {},
   "outputs": [],
   "source": [
    "animalList = ['mouse-data']\n",
    "\n",
    "animalFiles = []\n",
    "for animal in animalList:\n",
    "    animalFiles.extend(utility.find_file(root/animal, 'mat'))\n",
    "\n",
    "\n",
    "AllDFs=[]\n",
    "for fname in animalFiles:\n",
    "    print(fname)\n",
    "    df = dt.load_pyal_data(fname)\n",
    "    df['mouse'] = fname.split(os.sep)[-1][fname.split(os.sep)[-1].find('WR'):].split('_')[0]\n",
    "    df['file'] = fname.split(os.sep)[-1]\n",
    "    df = prep_general_mouse(df)\n",
    "    AllDFs.append(df)\n",
    "\n",
    "clear_output()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f3b43cd",
   "metadata": {},
   "source": [
    "# Tests on a single session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d06bd110",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = AllDFs[0]\n",
    "area = areas[0]\n",
    "field = area + '_rates'\n",
    "good_df = []\n",
    "for df in AllDFs:\n",
    "    if field not in df.columns: continue\n",
    "    good_df.append(df)\n",
    "\n",
    "\n",
    "AllData = dt.get_data_array(good_df, exec_epoch, area=area, model=n_components)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23334765",
   "metadata": {},
   "outputs": [],
   "source": [
    "AllData.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a32a784b",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# CCA comparison\n",
    "\n",
    "## 2 mice, same epoch\n",
    "\n",
    "load the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f918410b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# load the DFs\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "allDFs_M1 = []\n",
    "for df in AllDFs:\n",
    "    if 'M1_rates' in df.columns:\n",
    "        allDFs_M1.append(df)\n",
    "\n",
    "\n",
    "allDFs_Str = []\n",
    "for df in AllDFs:\n",
    "    if 'Str_rates' in df.columns:\n",
    "        allDFs_Str.append(df)\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "\n",
    "pairFileList1 = []\n",
    "for I, df1 in enumerate(allDFs_M1):\n",
    "    for J, (df2) in enumerate(allDFs_M1):\n",
    "        if J<=I or df1.mouse[0] == df2.mouse[0]: continue  # repetitions\n",
    "        pairFileList1.append((I,J))\n",
    "\n",
    "print(f'{len(pairFileList1)=}')\n",
    "\n",
    "\n",
    "pairFileList2 = []\n",
    "for I, df1 in enumerate(allDFs_Str):\n",
    "    for J, df2 in enumerate(allDFs_Str):\n",
    "        if J<=I or df1.mouse[0] == df2.mouse[0]: continue  # repetitions\n",
    "        pairFileList2.append((I,J))\n",
    "\n",
    "print(f'{len(pairFileList2)=}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f845cd",
   "metadata": {},
   "source": [
    "collecting all the data in a matrix, `AllData`: $sessions \\times targets \\times  trials \\times time \\times PCs$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ede970a",
   "metadata": {},
   "outputs": [],
   "source": [
    "side1df = [allDFs_M1[i] for i,_ in pairFileList1]\n",
    "side2df = [allDFs_M1[j] for _,j in pairFileList1]\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "AllData1 = dt.get_data_array(side1df, exec_epoch, area=areas[0], model=n_components)\n",
    "AllData2 = dt.get_data_array(side2df, exec_epoch, area=areas[0], model=n_components)\n",
    "_,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "print(f'{min_trials=}\\n{min_time=}')\n",
    "\n",
    "side1df = [allDFs_Str[i] for i,_ in pairFileList2]\n",
    "side2df = [allDFs_Str[j] for _,j in pairFileList2]\n",
    "AllData1_ = dt.get_data_array(side1df, exec_epoch, area=areas[1], model=n_components)\n",
    "AllData2_ = dt.get_data_array(side2df, exec_epoch, area=areas[1], model=n_components)\n",
    "_,_, min_trials_, min_time_,_ = np.min((AllData1_.shape,AllData2_.shape),axis=0)\n",
    "print(f'{min_trials_=}\\n{min_time_=}')\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "allCCs0=[]\n",
    "for sessionData1,sessionData2 in zip(AllData1,AllData2):\n",
    "    data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,n_components))\n",
    "    data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,n_components))\n",
    "    allCCs0.append(dt.canoncorr(data1, data2))\n",
    "allCCs0 = np.array(allCCs0).T\n",
    "\n",
    "allCCs1=[]\n",
    "for sessionData1,sessionData2 in zip(AllData1_,AllData2_):\n",
    "    data1 = np.reshape(sessionData1[:,:min_trials_,:min_time_,:], (-1,n_components))\n",
    "    data2 = np.reshape(sessionData2[:,:min_trials_,:min_time_,:], (-1,n_components))\n",
    "    allCCs1.append(dt.canoncorr(data1, data2))\n",
    "allCCs1 = np.array(allCCs1).T\n",
    "\n",
    "# plot\n",
    "_,ax = plt.subplots()\n",
    "utility.shaded_errorbar(ax, allCCs0, color='b', marker = 'o', label=f'{areas[0]} $n={allCCs0.shape[1]}$ sessions')\n",
    "utility.shaded_errorbar(ax, allCCs1, color='r', marker = 'o', label=f'{areas[1]} $n={allCCs1.shape[1]}$ sessions')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('modes')\n",
    "ax.legend()\n",
    "ax.set_ylabel('canonical correlation')\n",
    "ax.set_title('CCA --- across mice', usetex=True);\n",
    "\n",
    "_,axes = plt.subplots(ncols=2, figsize=(10,5))\n",
    "axes[0].plot(allCCs0[:4,:].mean(axis=0),'.')\n",
    "axes[1].plot(allCCs1[:4,:].mean(axis=0),'.')\n",
    "axes[0].set_ylabel('average canonical correlation')\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(f'CCA --- across mice {areas[i]} ', usetex=True);\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_xlabel('sessions')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "185fcf0c",
   "metadata": {},
   "source": [
    "### Overall Figures\n",
    "\n",
    "across monkey and within monkey together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61e5bcd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot\n",
    "fig,axes = plt.subplots(ncols=2, figsize=(11,5), dpi=100, sharey=True, sharex=True)\n",
    "\n",
    "utility.shaded_errorbar(axes[0], np.arange(1,n_components+1), allCCs0, color='b', marker = 'o', label=f'Across, $n={allCCs0.shape[1]}$')\n",
    "utility.shaded_errorbar(axes[1], np.arange(1,n_components+1), allCCs1, color='r', marker = 'o', label=f'Across, $n={allCCs1.shape[1]}$')\n",
    "\n",
    "utility.shaded_errorbar(axes[0], np.arange(1,n_components+1), CC_upper_bound_M1, color='cornflowerblue', marker = '<', ls='--', label=f'Within, $n={CC_upper_bound_M1.shape[1]}$')\n",
    "utility.shaded_errorbar(axes[1], np.arange(1,n_components+1), CC_upper_bound_Str, color='lightcoral', marker = '<', ls='--', label=f'Within, $n={CC_upper_bound_Str.shape[1]}$')\n",
    "\n",
    "utility.shaded_errorbar(axes[0], np.arange(1,n_components+1), CC_lower_bound_M1, color='gray', marker = '>', ls=':', label=f'Control, $n={CC_lower_bound_M1.shape[1]}$')\n",
    "utility.shaded_errorbar(axes[1], np.arange(1,n_components+1), CC_lower_bound_Str, color='gray', marker = '>', ls=':', label=f'Control, $n={CC_lower_bound_Str.shape[1]}$')\n",
    "\n",
    "\n",
    "for i,ax in enumerate(axes):\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_xlim([.5,n_components+.5])\n",
    "    ax.set_xlabel('Neural mode')\n",
    "    ax.set_title(f'{areas[i]}')\n",
    "    ax.legend()\n",
    "axes[0].set_ylabel('Canonical correlation')\n",
    "fig.suptitle('Alignment', fontsize = 'xx-large');\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dc6cecd4",
   "metadata": {},
   "source": [
    "comapring group correlations\n",
    ">Like the NN paper, _Fig. 4e_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ad5f866",
   "metadata": {},
   "outputs": [],
   "source": [
    "fig,axes = plt.subplots(ncols=2, figsize=(11,5), dpi=100)\n",
    "bins = np.arange(0,1,0.05)\n",
    "ax = axes[0]\n",
    "ax.hist(allCCs0[:4,:].mean(axis=0), bins=bins, density=True,\n",
    "        color='b', alpha=.8, label=f'across, $n={allCCs0[:4,:].mean(axis=0).shape[0]}$')\n",
    "ax.hist(CC_upper_bound_M1[:4,:].mean(axis=0), bins=bins, density=True,\n",
    "        color='cornflowerblue', alpha=.8, label=f'within, $n={CC_upper_bound_M1[:4,:].mean(axis=0).shape[0]}$')\n",
    "ax.hist(CC_lower_bound_M1[:4,:].mean(axis=0), bins=bins, density=True,\n",
    "        color='gray', alpha=.8, label=f'control, $n={CC_lower_bound_M1[:4,:].mean(axis=0).shape[0]}$')\n",
    "ax = axes[1]\n",
    "ax.hist(allCCs1[:4,:].mean(axis=0), bins=bins, density=True,\n",
    "        color='r', alpha=.8, label=f'across, $n={allCCs1[:4,:].mean(axis=0).shape[0]}$')\n",
    "ax.hist(CC_upper_bound_Str[:4,:].mean(axis=0), bins=bins, density=True,\n",
    "        color='lightcoral', alpha=.8, label=f'within, $n={CC_upper_bound_Str[:4,:].mean(axis=0).shape[0]}$')\n",
    "ax.hist(CC_lower_bound_Str[:4,:].mean(axis=0), bins=bins, density=True,\n",
    "        color='gray', alpha=.8, label=f'control, $n={CC_lower_bound_Str[:4,:].mean(axis=0).shape[0]}$')\n",
    "\n",
    "for i,ax in enumerate(axes):\n",
    "    ax.set_title(f'{areas[i]}')\n",
    "    ax.set_xlim([0,1])\n",
    "    ax.set_xlabel('Canonical correlation')\n",
    "    ax.yaxis.set_major_locator(MaxNLocator(integer=True))\n",
    "    ax.legend(loc=2)\n",
    "axes[0].set_ylabel(r'Normalised \\# of sessions')\n",
    "fig.suptitle('Average of top 4 CCs', fontsize = 'xx-large')\n",
    "fig.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18ff04c2",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "## one mouse, 2 epochs\n",
    "\n",
    "as an extra control"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "100fcea0",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "side1df = allDFs_M1\n",
    "AllData1 = dt.get_data_array(side1df, prep_epoch, area=areas[0], model=n_components)\n",
    "AllData2 = dt.get_data_array(side1df, exec_epoch, area=areas[0], model=n_components)\n",
    "_,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "print(f'{min_trials=}\\n{min_time=}')\n",
    "\n",
    "# PMd\n",
    "side2df = allDFs_Str\n",
    "AllData1_ = dt.get_data_array(side2df, prep_epoch, area=areas[1], model=n_components)\n",
    "AllData2_ = dt.get_data_array(side2df, exec_epoch, area=areas[1], model=n_components)\n",
    "_,_, min_trials_, min_time_,_ = np.min((AllData1_.shape,AllData2_.shape),axis=0)\n",
    "print(f'{min_trials_=}\\n{min_time_=}')\n",
    "\n",
    "inCCs0=[]\n",
    "for sessionData1,sessionData2 in zip(AllData1,AllData2):\n",
    "    data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,n_components))\n",
    "    data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,n_components))\n",
    "    inCCs0.append(dt.canoncorr(data1, data2))\n",
    "inCCs0 = np.array(inCCs0).T\n",
    "\n",
    "inCCs1=[]\n",
    "for sessionData1,sessionData2 in zip(AllData1_,AllData2_):\n",
    "    data1 = np.reshape(sessionData1[:,:min_trials_,:min_time_,:], (-1,n_components))\n",
    "    data2 = np.reshape(sessionData2[:,:min_trials_,:min_time_,:], (-1,n_components))\n",
    "    inCCs1.append(dt.canoncorr(data1, data2))\n",
    "inCCs1 = np.array(inCCs1).T\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "# plotting\n",
    "_,ax = plt.subplots()\n",
    "\n",
    "utility.shaded_errorbar(ax, inCCs0, color='b', marker = 'o', label=f'{areas[0]} $n={inCCs0.shape[1]}$ sessions')\n",
    "utility.shaded_errorbar(ax, inCCs1, color='r', marker = 'o', label=f'{areas[1]} $n={inCCs1.shape[1]}$ sessions')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('components')\n",
    "ax.legend()\n",
    "ax.set_ylabel('canonical correlation')\n",
    "ax.set_title('CCA --- within mouse', usetex=True);\n",
    "\n",
    "fig,axes = plt.subplots(ncols=2, figsize=(10,5))\n",
    "axes[0].plot(inCCs0[:4,:].mean(axis=0),'.')\n",
    "axes[1].plot(inCCs1[:4,:].mean(axis=0),'.')\n",
    "axes[0].set_ylabel('average canonical correlation')\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(f'CCA --- within mouse {areas[i]} ', usetex=True);\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_xlabel('sessions')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0376f9be",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "# VAF?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91f48362",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_unit_data_and_pca(data_list: list[pd.DataFrame], epoch , area: str ='M1', n_components: int =10) -> tuple[np.ndarray, sklearn.decomposition._pca.PCA]:\n",
    "    \"\"\"\n",
    "    Applies PCA to the data and return a data matrix of the shape: sessions x targets x  trials x time x PCs\n",
    "    with the minimum number of trials and timepoints shared across all the datasets/targets.\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    `data_list`: list of pd.dataFrame datasets from pyal-data\n",
    "    `epoch`: an epoch function of the type `pyal.generate_epoch_fun`\n",
    "    `area`: area, either: 'M1', or 'S1', or 'PMd'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `AllData`: np.array\n",
    "    `AllPCA`: list of pca objects of each session\n",
    "\n",
    "    Signature\n",
    "    -------\n",
    "    AllData = get_data_array(data_list, execution_epoch, area='M1', n_components=10)\n",
    "    all_data = np.reshape(AllData, (-1,10))\n",
    "    \"\"\"\n",
    "    field = f'{area}_rates'\n",
    "    n_shared_trial = np.inf\n",
    "    n_unit = np.inf\n",
    "    for df in data_list:\n",
    "        n_unit = np.min((df[field][0].shape[1], n_unit))\n",
    "        for target in range(8):\n",
    "            df_ = pyal.select_trials(df, df.target_id== target)\n",
    "            n_shared_trial = np.min((df_.shape[0], n_shared_trial))\n",
    "\n",
    "    n_shared_trial = int(n_shared_trial)\n",
    "    n_unit = int(n_unit)\n",
    "\n",
    "    # finding the number of timepoints\n",
    "    df_ = pyal.restrict_to_interval(df_,epoch_fun=epoch)\n",
    "    n_timepoints = int(df_[field][0].shape[0])\n",
    "\n",
    "    # pre-allocating the data matrix\n",
    "    AllData = np.empty((len(data_list), 8, n_shared_trial, n_timepoints, n_unit))\n",
    "    AllPCA = []\n",
    "    rng = np.random.default_rng(12345)\n",
    "    for session, df in enumerate(data_list):\n",
    "        df_ = pyal.restrict_to_interval(df, epoch_fun=epoch)\n",
    "        rates = np.concatenate(df_[field].values, axis=0)\n",
    "        rates -= np.mean(rates, axis=0)\n",
    "        all_units = np.arange(rates.shape[1])\n",
    "        rng.shuffle(all_units)\n",
    "        rates_model = PCA(n_components=n_components, svd_solver='full').fit(rates[:,all_units[:n_unit]])\n",
    "        AllPCA.append(rates_model)\n",
    "        \n",
    "        for target in range(8):\n",
    "            df__ = pyal.select_trials(df_, df_.target_id==target)\n",
    "            all_id = df__.trial_id.to_numpy()\n",
    "            rng.shuffle(all_id)\n",
    "            # select the right number of trials to each target\n",
    "            df__ = pyal.select_trials(df__, lambda trial: trial.trial_id in all_id[:n_shared_trial])\n",
    "            for trial, trial_rates in enumerate(df__[field]):\n",
    "                AllData[session,target,trial, :, :] = trial_rates [:,all_units[:n_unit]]\n",
    "    \n",
    "    return AllData, AllPCA\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06584765",
   "metadata": {},
   "source": [
    "## 2 monkeys, same epoch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "006bb9f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pairFileList1 = []\n",
    "for I, animal1 in enumerate(GoodDataList[areas[0]]):\n",
    "    for J, animal2 in enumerate(GoodDataList[areas[0]]):\n",
    "        if J<=I or '2' in animal1+animal2:  # to repetitions and to remove Chewie2\n",
    "            continue\n",
    "        path1List = [root/animal1/GoodDataList[areas[0]][animal1][i] for i,_ in enumerate(GoodDataList[areas[0]][animal1])]\n",
    "        path2List = [root/animal2/GoodDataList[areas[0]][animal2][i] for i,_ in enumerate(GoodDataList[areas[0]][animal2])]\n",
    "        for path1 in path1List:\n",
    "            df1 = dt.load_pyal_data(path1)\n",
    "            for path2 in path2List:\n",
    "                pairFileList1.append((df1, dt.load_pyal_data(path2)))\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "pairFileList_area0 = [(prep_general(df1),prep_general(df2)) for  df1,df2 in pairFileList1]\n",
    "del pairFileList1\n",
    "gc.collect()\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "side1df = [df for df,_ in pairFileList_area0]\n",
    "side2df = [df for _,df in pairFileList_area0]\n",
    "AllData1, AllPca1 = get_unit_data_and_pca(side1df, exec_epoch, area=areas[0], n_components=n_components)\n",
    "AllData2, AllPca2 = get_unit_data_and_pca(side2df, exec_epoch, area=areas[0], n_components=n_components)\n",
    "_,_, min_trials, min_time, _ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "\n",
    "\n",
    "allVAFs=[]\n",
    "for sessionData1,sessionData2, model1, model2 in zip(AllData1, AllData2, AllPca1, AllPca2):\n",
    "    data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,sessionData1.shape[-1]))\n",
    "    data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,sessionData2.shape[-1]))\n",
    "    A, B, *_ = dt.canoncorr(model1.transform(data1), model2.transform(data2), fullReturn=True)\n",
    "    allVAFs.append(dt.VAF_pc_cc(X=data1, C=model1.components_, A=A)+dt.VAF_pc_cc(X=data2, C=model2.components_, A=B))\n",
    "allVAFs = np.cumsum(np.array(allVAFs).T /2, axis=0)\n",
    "\n",
    "\n",
    "# plot\n",
    "_,ax = plt.subplots()\n",
    "utility.shaded_errorbar(ax, allVAFs, color='b', marker = 'o', label=f'{areas[0]} $n={allVAFs.shape[1]}$ sessions')\n",
    "ax.plot(allVAFs,lw=1, alpha=.3);\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('axes')\n",
    "ax.legend()\n",
    "ax.set_ylabel('VAF')\n",
    "ax.set_title('VAF --- M1 across monkeys', usetex=True);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4712073",
   "metadata": {},
   "outputs": [],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "pairFileList2 = []\n",
    "for I, animal1 in enumerate(GoodDataList[areas[1]]):\n",
    "    for J, animal2 in enumerate(GoodDataList[areas[1]]):\n",
    "        if J<=I or '2' in animal1+animal2:  # to repetitions and to remove Chewie2\n",
    "            continue\n",
    "        path1List = [root/animal1/GoodDataList[areas[1]][animal1][i] for i,_ in enumerate(GoodDataList[areas[1]][animal1])]\n",
    "        path2List = [root/animal2/GoodDataList[areas[1]][animal2][i] for i,_ in enumerate(GoodDataList[areas[1]][animal2])]\n",
    "        for path1 in path1List:\n",
    "            df1 = dt.load_pyal_data(path1)\n",
    "            for path2 in path2List:\n",
    "                pairFileList2.append((df1, dt.load_pyal_data(path2)))\n",
    "\n",
    "\n",
    "gc.collect()\n",
    "pairFileList_area1 = [(prep_general(df1),prep_general(df2)) for  df1,df2 in pairFileList2]\n",
    "del pairFileList2\n",
    "gc.collect()\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "side1df = [df for df,_ in pairFileList_area1]\n",
    "side2df = [df for _,df in pairFileList_area1]\n",
    "AllData1, AllPca1 = get_unit_data_and_pca(side1df, exec_epoch, area=areas[1], n_components=n_components)\n",
    "AllData2, AllPca2 = get_unit_data_and_pca(side2df, exec_epoch, area=areas[1], n_components=n_components)\n",
    "_,_, min_trials, min_time, _ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "\n",
    "\n",
    "allVAFs2=[]\n",
    "for sessionData1,sessionData2, model1, model2 in zip(AllData1, AllData2, AllPca1, AllPca2):\n",
    "    data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,sessionData1.shape[-1]))\n",
    "    data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,sessionData2.shape[-1]))\n",
    "    A, B, *_ = dt.canoncorr(model1.transform(data1), model2.transform(data2), fullReturn=True)\n",
    "    allVAFs2.append(dt.VAF_pc_cc(X=data1, C=model1.components_, A=A)+dt.VAF_pc_cc(X=data2, C=model2.components_, A=B))\n",
    "allVAFs2 = np.cumsum(np.array(allVAFs2).T / 2, axis=0)\n",
    "\n",
    "\n",
    "# plot\n",
    "_,ax = plt.subplots()\n",
    "utility.shaded_errorbar(ax, allVAFs2, color='b', marker = 'o', label=f'{areas[1]} $n={allVAFs.shape[1]}$ sessions')\n",
    "ax.plot(allVAFs2,lw=1, alpha=.3);\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('axes')\n",
    "ax.legend()\n",
    "ax.set_ylabel('VAF')\n",
    "ax.set_title('VAF --- PMd across monkeys', usetex=True);"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28dbb10d",
   "metadata": {},
   "source": [
    "## one monkey, two epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bcc5d30",
   "metadata": {},
   "outputs": [],
   "source": [
    "pairFileList1 = []\n",
    "for animal1 in GoodDataList[areas[0]]:\n",
    "    path1List = [root/animal1/GoodDataList[areas[0]][animal1][i] for i,_ in enumerate(GoodDataList[areas[0]][animal1])]\n",
    "    for path1 in path1List:\n",
    "        df1 = dt.load_pyal_data(path1)\n",
    "        pairFileList1.append(df1)\n",
    "\n",
    "print(f'{len(pairFileList1)=}')\n",
    "\n",
    "pairFileList2 = []\n",
    "for animal1 in GoodDataList[areas[1]]:\n",
    "    path1List = [root/animal1/GoodDataList[areas[1]][animal1][i] for i,_ in enumerate(GoodDataList[areas[1]][animal1])]\n",
    "    for path1 in path1List:\n",
    "        df1 = dt.load_pyal_data(path1)\n",
    "        pairFileList2.append(df1)\n",
    "\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "gc.collect()\n",
    "pairFileList_area0 = [prep_general(df) for df in pairFileList1]\n",
    "del pairFileList1\n",
    "gc.collect()\n",
    "pairFileList_area1 = [prep_general(df) for  df in pairFileList2]\n",
    "del pairFileList2\n",
    "gc.collect()\n",
    "\n",
    "\n",
    "side1df = pairFileList_area0\n",
    "AllData1, AllPca1 = get_unit_data_and_pca(side1df, prep_epoch, area=areas[0], n_components=n_components)\n",
    "AllData2, AllPca2 = get_unit_data_and_pca(side1df, exec_epoch, area=areas[0], n_components=n_components)\n",
    "_,_, min_trials, min_time,_ = np.min((AllData1.shape,AllData2.shape),axis=0)\n",
    "print(f'{min_trials=}\\n{min_time=}')\n",
    "# PMd\n",
    "side2df = pairFileList_area1\n",
    "AllData1_, AllPca1_ = get_unit_data_and_pca(side2df, prep_epoch, area=areas[1], n_components=n_components)\n",
    "AllData2_, AllPca2_ = get_unit_data_and_pca(side2df, exec_epoch, area=areas[1], n_components=n_components)\n",
    "_,_, min_trials_, min_time_,_ = np.min((AllData1_.shape,AllData2_.shape),axis=0)\n",
    "print(f'{min_trials_=}\\n{min_time_=}')\n",
    "\n",
    "inVAFs0=[]\n",
    "for sessionData1,sessionData2,model1,model2 in zip(AllData1,AllData2,AllPca1,AllPca2):\n",
    "    data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,sessionData1.shape[-1]))\n",
    "    data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,sessionData2.shape[-1]))\n",
    "    A, B, *_ = dt.canoncorr(model1.transform(data1), model2.transform(data2), fullReturn=True)\n",
    "    inVAFs0.append(dt.VAF_pc_cc(X=data1, C=model1.components_, A=A)+dt.VAF_pc_cc(X=data2, C=model2.components_, A=B))\n",
    "inVAFs0 = np.cumsum(np.array(inVAFs0).T / 2, axis=0)\n",
    "\n",
    "inVAFs1=[]\n",
    "for sessionData1,sessionData2,model1,model2 in zip(AllData1_,AllData2_,AllPca1_,AllPca2_):\n",
    "    data1 = np.reshape(sessionData1[:,:min_trials,:min_time,:], (-1,sessionData1.shape[-1]))\n",
    "    data2 = np.reshape(sessionData2[:,:min_trials,:min_time,:], (-1,sessionData2.shape[-1]))\n",
    "    A, B, *_ = dt.canoncorr(model1.transform(data1), model2.transform(data2), fullReturn=True)\n",
    "    inVAFs1.append(dt.VAF_pc_cc(X=data1, C=model1.components_, A=A)+dt.VAF_pc_cc(X=data2, C=model2.components_, A=B))\n",
    "inVAFs1 = np.cumsum(np.array(inVAFs1).T / 2, axis=0)\n",
    "warnings.filterwarnings(\"default\")\n",
    "\n",
    "\n",
    "# plotting\n",
    "_,ax = plt.subplots()\n",
    "\n",
    "utility.shaded_errorbar(ax, inVAFs0, color='b', marker = 'o', label=f'{areas[0]} $n={inVAFs0.shape[1]}$ sessions')\n",
    "utility.shaded_errorbar(ax, inVAFs1, color='r', marker = 'o', label=f'{areas[1]} $n={inVAFs1.shape[1]}$ sessions')\n",
    "ax.set_ylim([0,1])\n",
    "ax.set_xlabel('axes')\n",
    "ax.legend()\n",
    "ax.set_ylabel('VAF')\n",
    "ax.set_title('VAF --- within monkeys', usetex=True);\n",
    "\n",
    "fig,axes = plt.subplots(ncols=2, figsize=(10,5))\n",
    "utility.shaded_errorbar(axes[0], allVAFs, color='b', marker = 'o', label=f'Across monkeys $n={allVAFs.shape[1]}$ sessions')\n",
    "utility.shaded_errorbar(axes[0], inVAFs0, color='r', marker = 'o', label=f'Within monkeys $n={inVAFs0.shape[1]}$ sessions')\n",
    "utility.shaded_errorbar(axes[1], allVAFs2, color='b', marker = 'o', label=f'Across monkeys $n={allVAFs2.shape[1]}$ sessions')\n",
    "utility.shaded_errorbar(axes[1], inVAFs1,  color='r', marker = 'o', label=f'Within monkeys $n={inVAFs1.shape[1]}$ sessions')\n",
    "\n",
    "axes[0].set_ylabel('average VAF')\n",
    "for i, ax in enumerate(axes):\n",
    "    ax.set_title(f'VAF --- {areas[i]} ', usetex=True);\n",
    "    ax.set_ylim([0,1])\n",
    "    ax.set_xlabel('CC axes')\n",
    "    ax.legend()"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72da6499f934495e06c03d484049d4696c0f7b78c6b9c64cf8676e9ec2014a6a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
