{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d374a142",
   "metadata": {},
   "source": [
    "# part0: imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05c18582",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "from pprint import pprint \n",
    "from importlib import reload\n",
    "import logging\n",
    "from typing import Callable\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.linalg as linalg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "from tools import utilityTools as utility\n",
    "from tools import dataTools as dt\n",
    "import pyaldata as pyal\n",
    "\n",
    "%matplotlib inline\n",
    "reload(dt)\n",
    "\n",
    "# Global params\n",
    "root = pathlib.Path(\"/data\")\n",
    "\n",
    "BIN_SIZE = .03  # sec\n",
    "WINDOW_prep = (-.39, .06)  # sec\n",
    "WINDOW_exec = (-.12, .42)  # sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8197db3d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_target_id(trial):\n",
    "    return int(np.round((trial.target_direction + np.pi) / (0.25*np.pi))) - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77d24ba1-312e-4789-956a-764709bde983",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run dataset_selection.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "112c9354",
   "metadata": {},
   "source": [
    "## Control\n",
    "\n",
    "let's find a lower-bound. Canonical correlation without matching the animal, the area, the epoch and the target!\n",
    "\n",
    "### Details\n",
    "- **animal**: $C_L$  is Chewie\n",
    "- **window**: For *M1* it is $-120ms \\sim +420ms$ and for *PMd* from $-390ms  \\sim +60ms$\n",
    "- **dim**: it is 10 for M1 and 15 for PMd\n",
    "- **preprocessing**: \n",
    "    - remove firing rate below 1Hz overall\n",
    "    - bin to 30ms\n",
    "    - square root transform\n",
    "    - smooth by Gaussian kernel, $\\sigma=50ms$\n",
    "    \n",
    "I'm including only one session per animal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13dfe879-d73f-45b7-8e23-191971780fc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "areas = ('M1', 'PMd')\n",
    "\n",
    "pairFileList = []\n",
    "for animal1 in GoodDataList[areas[0]]:\n",
    "    for animal2 in GoodDataList[areas[1]]:\n",
    "        if animal2  == animal1 or '2' in animal1 or '2' in animal2:  # to removre Chewie2\n",
    "            continue\n",
    "        path1 = root/animal1/GoodDataList[areas[0]][animal1][0]\n",
    "        path2 = root/animal2/GoodDataList[areas[1]][animal2][0]\n",
    "        pairFileList.append((pyal.mat2dataframe(path1, shift_idx_fields=True),\n",
    "                             pyal.mat2dataframe(path2, shift_idx_fields=True)))\n",
    "        \n",
    "print(f'{len(pairFileList)=}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f704107-1afa-4532-bab9-4a56af357967",
   "metadata": {},
   "outputs": [],
   "source": [
    "def prep_general (df, area='M1'):\n",
    "    \"preprocessing general!\"\n",
    "    \n",
    "    df[\"target_id\"] = df.apply(get_target_id, axis=1)  # add a field `target_id` with int values\n",
    "\n",
    "    df_ = pyal.remove_low_firing_neurons(df, f\"{area}_spikes\", 1)\n",
    "    \n",
    "    df_= pyal.select_trials(df, df.result== 'R')\n",
    "    df_= pyal.select_trials(df_, df_.epoch=='BL')\n",
    "    \n",
    "    assert np.all(df_data_M1[0].bin_size == .01), 'bin size is not consistent!'\n",
    "    df_ = pyal.combine_time_bins(df_, int(BIN_SIZE/.01))\n",
    "    \n",
    "    df_ = pyal.sqrt_transform_signal(df_, f\"{area}_spikes\")\n",
    "        \n",
    "    df_= pyal.add_firing_rates(df_, 'smooth', std=0.05)\n",
    "    \n",
    "    \n",
    "    return df_\n",
    "\n",
    "\n",
    "df_M1_ready = [prep_general(df) for  df in df_data_M1]\n",
    "\n",
    "prep_epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on',\n",
    "                                     rel_start=int(WINDOW_prep[0]/BIN_SIZE),\n",
    "                                     rel_end=int(WINDOW_prep[1]/BIN_SIZE)\n",
    "                                    )\n",
    "exec_epoch = pyal.generate_epoch_fun(start_point_name='idx_movement_on', \n",
    "                                     rel_start=int(WINDOW_exec[0]/BIN_SIZE),\n",
    "                                     rel_end=int(WINDOW_exec[1]/BIN_SIZE)\n",
    "                                    )"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f1f9231-9442-466d-a6c8-b526dd00d182",
   "metadata": {},
   "source": [
    "Finding the minimum number of trials per target across all datasets\n",
    "\n",
    "and some other parameters"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65765da1-7e94-4b3a-8ef3-032b0f6bceb8",
   "metadata": {},
   "source": [
    "collecting all the data in a matrix, `AllData`: $sessions \\times targets \\times  trials \\times time \\times PCs$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a56d7a28-ce26-4dfb-945e-a352700ef903",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_array(data_list: list, epoch , area: str ='M1', n_components: int =10) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Applies PCA to the data and return a data matrix of the shape: sessions x targets x  trials x time x PCs\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    `data_list`: list of pd.dataFrame datasets from pyal-data\n",
    "    `epoch`: an epoch function of the type `pyal.generate_epoch_fun`\n",
    "    \n",
    "    `area`: area, either: 'M1', or 'S1', or 'PMd'\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    `AllData`: np.array\n",
    "\n",
    "    Signature\n",
    "    -------\n",
    "    AllData = get_data_array(data_list, execution_epoch, n_components=10)\n",
    "    all_data = np.reshape(AllData, (-1,10))\n",
    "    \"\"\"\n",
    "    field = f'{area}_rates'\n",
    "    n_shared_trial = np.inf\n",
    "    for df in data_list:\n",
    "        for target in range(8):\n",
    "            df_ = pyal.select_trials(df, df.target_id== target)\n",
    "            n_shared_trial = np.min((df_.shape[0], n_shared_trial))\n",
    "\n",
    "    n_shared_trial = int(n_shared_trial)\n",
    "\n",
    "    # finding the number of timepoints\n",
    "    df_ = pyal.restrict_to_interval(df_,epoch_fun=epoch)\n",
    "    n_timepoints = int(df_[field][0].shape[0])\n",
    "\n",
    "    # pre-allocating the data matrix\n",
    "    AllData = np.empty((len(data_list), 8, n_shared_trial, n_timepoints, n_components))\n",
    "\n",
    "    rng = np.random.default_rng(12345)\n",
    "    for session, df in enumerate(data_list):\n",
    "        df_ = pyal.restrict_to_interval(df,epoch_fun=epoch)\n",
    "        rates = np.concatenate(df_[field].values, axis=0)\n",
    "        rates -= np.mean(rates,axis=0)\n",
    "        rates_model = PCA(n_components=n_components, svd_solver='full').fit(rates)\n",
    "        df_ = pyal.apply_dim_reduce_model(df_, rates_model, field, '_pca');\n",
    "\n",
    "        for target in range(8):\n",
    "            df__ = pyal.select_trials(df_, df_.target_id==target)\n",
    "            all_id = df__.trial_id.to_numpy()\n",
    "            rng.shuffle(all_id)\n",
    "            # select the right number of trials to each target\n",
    "            df__ = pyal.select_trials(df__, lambda trial: trial.trial_id in all_id[:n_shared_trial])\n",
    "            for trial, trial_rates in enumerate(df__._pca):\n",
    "                AllData[session,target,trial, :, :] = trial_rates\n",
    "    \n",
    "    return AllData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef9af5e5-94be-4e5e-a155-ff8011ee595b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0abdc54-90af-4fcf-ade7-ef40f5c3c972",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96e1d90e-8f3e-45cb-9b00-e9fd33d3b094",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f1651cb-a548-4a94-924a-32ca008ad61e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "405ffa3a-1675-47ba-bf44-5da36058546c",
   "metadata": {},
   "source": [
    "do the CCA calculation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00dfc176-d59d-4a1c-a098-0762f6ffb008",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "trialList = np.arange(n_shared_trial)\n",
    "\n",
    "CCs=[]\n",
    "for session, sessionData in enumerate(AllData):\n",
    "    r = []\n",
    "    for n in range(n_iter):\n",
    "        rng.shuffle(trialList)\n",
    "        # non-overlapping randomised trials\n",
    "        trial1 = trialList[:n_shared_trial//2]\n",
    "        trial2 = trialList[-n_shared_trial//2:-1]\n",
    "        data1 = np.reshape(sessionData[:,trial1,:,:], (-1,n_components))\n",
    "        data2 = np.reshape(sessionData[:,trial2,:,:], (-1,n_components))\n",
    "        r.append(np.mean(dt.canoncorr(data1, data2)[:4]))\n",
    "    CCs.append(r)\n",
    "CCs = np.array(CCs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "79a9a048-578f-4a0c-8ecb-ff9aec6c8f42",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(CCs.T,'.')\n",
    "plt.ylim([0,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc1013e6-084b-4cba-a915-adeb75b1e908",
   "metadata": {},
   "source": [
    "the average distribution of CCs for each session"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54b8d3ff-d161-440e-9d85-10692732c123",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_iter = 100\n",
    "trialList = np.arange(n_shared_trial)\n",
    "\n",
    "CCs=[]\n",
    "for session, sessionData in enumerate(AllData):\n",
    "    r = []\n",
    "    for n in range(n_iter):\n",
    "        rng.shuffle(trialList)\n",
    "        # non-overlapping randomised trials\n",
    "        trial1 = trialList[:n_shared_trial//2]\n",
    "        trial2 = trialList[-n_shared_trial//2:-1]\n",
    "        data1 = np.reshape(sessionData[:,trial1,:,:], (-1,n_components))\n",
    "        data2 = np.reshape(sessionData[:,trial2,:,:], (-1,n_components))\n",
    "        r.append(dt.canoncorr(data1, data2))\n",
    "    CCs.append(np.array(r))\n",
    "CCs = np.array(CCs).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2141760e-50b8-4ba8-b159-503a5a2a1379",
   "metadata": {},
   "outputs": [],
   "source": [
    "for i in range(4):\n",
    "    plt.plot(np.mean(CCs[:,:,i],axis=1))"
   ]
  }
 ],
 "metadata": {
  "interpreter": {
   "hash": "72da6499f934495e06c03d484049d4696c0f7b78c6b9c64cf8676e9ec2014a6a"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
