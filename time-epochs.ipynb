{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "international-season",
   "metadata": {},
   "source": [
    "# part0: imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "treated-driving",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, sys, pathlib\n",
    "from pprint import pprint \n",
    "from importlib import reload\n",
    "import logging\n",
    "logging.basicConfig(level=logging.ERROR)\n",
    "import warnings\n",
    "warnings.simplefilter(\"ignore\")\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "from sklearn.decomposition import PCA\n",
    "import scipy.linalg as linalg\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.cm as cm\n",
    "import matplotlib\n",
    "from matplotlib.ticker import MaxNLocator\n",
    "\n",
    "\n",
    "from tools import utilityTools as utility\n",
    "from tools import dataTools as dt\n",
    "import pyaldata as pyal\n",
    "\n",
    "%matplotlib inline\n",
    "\n",
    "root = pathlib.Path(\"/data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "unnecessary-chamber",
   "metadata": {},
   "source": [
    "# Compare different epochs\n",
    "\n",
    "the idea is to see whether canonical axes between 2 animals provide a higher VAF for time epochs in the trial that they have not been trained on, compared to, for example, M1-PMd axes in a single animal.\n",
    "\n",
    "\n",
    "## VAF in a subspace\n",
    "\n",
    "Check the VAF for a single session as a test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "d988006d-de3a-4f4d-a8a1-e5829d982d1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "animal= 'Chewie'\n",
    "fname = root / animal / \"Chewie_CO_CS_2016-10-21.mat\"\n",
    "\n",
    "df = pyal.mat2dataframe(fname, shift_idx_fields=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "229e78c4-980b-4e3f-a5a9-74fd1af4ae96",
   "metadata": {},
   "source": [
    "preprocessing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d5b3192c-8844-4bb2-8177-b6000a930086",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/mostafa/Repositories/MyPyalData/pyaldata/tools.py:940: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n",
      "/home/mostafa/Repositories/MyPyalData/pyaldata/tools.py:940: UserWarning: Assuming spikes are actually spikes and dividing by bin size.\n",
      "  utils.warnings.warn(\"Assuming spikes are actually spikes and dividing by bin size.\")\n"
     ]
    }
   ],
   "source": [
    "def get_target_id(trial):\n",
    "    return int(np.round((trial.target_direction + np.pi) / (0.25*np.pi))) - 1\n",
    "\n",
    "df = pyal.select_trials(df, df.result== 'R')\n",
    "df = pyal.select_trials(df, df.epoch=='BL')\n",
    "df = pyal.remove_low_firing_neurons(df, \"M1_spikes\", 1)\n",
    "df = pyal.remove_low_firing_neurons(df, \"PMd_spikes\", 1)\n",
    "\n",
    "df = pyal.add_firing_rates(df, 'smooth')\n",
    "\n",
    "df[\"target_id\"] = df.apply(get_target_id, axis=1)  # add a field `target_id` with int values"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c5f0b22-953d-44de-af2b-87c8eed2cb6e",
   "metadata": {},
   "source": [
    "applying PCA for 300ms post movement onset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "a2b93f5a-47c0-4771-b53c-640e8aa78355",
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ = pyal.restrict_to_interval(df, start_point_name='idx_movement_on', rel_start=0, rel_end=30)\n",
    "\n",
    "M1_rates = np.concatenate(df_.M1_rates.values, axis=0)\n",
    "M1_rates -=np.mean(M1_rates,axis=0)\n",
    "\n",
    "M1_model = PCA(n_components=10, svd_solver='full');\n",
    "M1_model.fit(M1_rates);\n",
    "df_ = pyal.apply_dim_reduce_model(df_, M1_model, 'M1_rates', 'M1_pca');\n",
    "\n",
    "\n",
    "PMd_rates = np.concatenate(df_.PMd_rates.values, axis=0)\n",
    "PMd_rates -=np.mean(PMd_rates,axis=0)\n",
    "\n",
    "PMd_model = PCA(n_components=10, svd_solver='full');\n",
    "PMd_model.fit(PMd_rates);\n",
    "df_ = pyal.apply_dim_reduce_model(df_, PMd_model, 'PMd_rates', 'PMd_pca');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8747fc8d-6c8b-49ad-aba2-0fe05c8f0dff",
   "metadata": {},
   "source": [
    "to make life easy, just limit everything to 1 target"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "87f9eb87-2113-4323-873d-bc80a832bbaf",
   "metadata": {},
   "outputs": [],
   "source": [
    "df__= pyal.select_trials(df_, df_.target_id ==3)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ff733e-c52d-4aa4-a890-eed01d1978b6",
   "metadata": {},
   "source": [
    "CCA on m1-pmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e075a360-3458-4b3c-a319-c41f6f7f20f2",
   "metadata": {},
   "outputs": [],
   "source": [
    "d0 = np.concatenate(df__['M1_pca'].values, axis=0)\n",
    "d1 = np.concatenate(df__['PMd_pca'].values, axis=0)\n",
    "\n",
    "n_samples = min ([d0.shape[0], d1.shape[0]])\n",
    "d0 = d0[:n_samples,:]\n",
    "d1 = d1[:n_samples,:]\n",
    "\n",
    "A, B, r, _, _ = dt.canoncorr(d0, d1, fullReturn=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "716574b0-0ef3-4c1d-92f5-3e1c28d0bd99",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the CCs are:[0.9246411  0.79977804 0.67285567 0.56713524 0.49359452 0.36927065\n",
      " 0.34098189 0.25425819 0.13815213 0.04389754]\n"
     ]
    }
   ],
   "source": [
    "print(f'the CCs are:{r}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "932b3a1b-714d-4d6e-b1b0-958f9223bf3a",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "From Bence's code [here](https://github.com/BeNeuroLab/pysubspaces/blob/4b7c7512ccdff8fdbd1334cb6c3acc8f5bdbf3d6/pysubspaces/subspaces.py#L29)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "3866193f-f1fb-4b7c-a16e-b16a8bf89731",
   "metadata": {},
   "outputs": [],
   "source": [
    "def variance_in_subspace(X, W):\n",
    "    \"\"\"\n",
    "    Variance in a given subspace\n",
    "    Parameters\n",
    "    ----------\n",
    "    X : 2D np.ndarray\n",
    "        n_samples x n_features data array\n",
    "    W : 2D np.ndarray\n",
    "        n_components x n_features projection matrix\n",
    "    Returns\n",
    "    -------\n",
    "    variance in the subspace\n",
    "    \"\"\"\n",
    "    return np.trace(W @ np.cov(X.T) @ W.T)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4b182baa-f764-4bf5-9659-efadd053c777",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(10.000000000000002, 9.999999999999996)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "variance_in_subspace (d0, A.T), variance_in_subspace (d1, B.T)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c03ef027-0e04-4ee0-ab36-49c4fb1737ff",
   "metadata": {},
   "source": [
    "weirdly specific?! but what is the total variance? what does it mean?\n",
    "- can I shuffle the weights in the prrojection matrix and repeat mulriple times to estimate a band of VAF by chance?\n",
    "    - maybe, check the matlab code\n",
    "- this is probably based on [this paper](https://www.nature.com/articles/ncomms13239#Sec9).\n",
    "\n",
    "---\n",
    "\n",
    "Based on the Gallego Nat Comm 2018 paper and some playing (data and projection matrices in the paper are row-wise): \n",
    "$$\\%VAF=\\frac{norm(X)-norm(X-XC^TA(A^TA)^{-1}A^TC)}{norm(X)}$$\n",
    "where:\n",
    "- $A$ is the CCA output, the output of the `canoncorr` function\n",
    "- $C$ is the `PCA_model.components_`\n",
    "- $X$ is the data matrix, $T\\times n$ with $T$ time points and $n$ neurons, and each neuron is **zero mean**\n",
    "- $norm$ is sum of squared elements  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "0ccfab0d-2b80-4afc-8a07-f7cf79f228de",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6482088310037387"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "C = M1_model.components_\n",
    "X = M1_rates\n",
    "norm = lambda m:np.sum(m**2)\n",
    "\n",
    "VAF = norm(X) - norm(X-X@C.T@A@linalg.inv(A.T@A)@A.T@C)\n",
    "VAF /= norm(X)\n",
    "\n",
    "VAF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9825c60a-1686-45f6-92b3-6c92f8fff6b7",
   "metadata": {},
   "source": [
    "Seems to give reasonable results: lowering number of PCs reduces the VAF.\n",
    "\n",
    "However, there are problemss:\n",
    "- the formulation in the paper, gives a VAF for each dimension, so the formula above should be applied to individual axes.\n",
    "- It is already implemented in MATLAB [here](https://github.com/limblab/proc-juan/blob/65dd81a1fc0ddb23def8039df9053a2be3e9bf88/paper_cross-task_manifold_review/vaf_CCs.m#L114), which uses the VAF function from the dPCA paper [here](https://github.com/machenslab/dPCA/blob/master/matlab/dpca_explainedVariance.m). I should translate that to compare?!?\n",
    "\n",
    "\n",
    "### dimension-wise VFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aca0e7a5-b287-4435-a167-7ee4a5ca9c81",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 0.01229023  0.10043615  0.06834629 -0.00385621  0.05905853  0.01711739\n",
      " -0.02952448 -0.17102626 -0.01291792  0.01157017] \n",
      " sums up to: 0.05149389724350406\n"
     ]
    }
   ],
   "source": [
    "D = linalg.inv(A.T@A)@A.T@C\n",
    "E = C.T@A\n",
    "\n",
    "VAFs=np.empty((C.shape[0],))\n",
    "\n",
    "for comp in range(C.shape[0]):\n",
    "\n",
    "    VAF = norm(X - X @ E[:,comp:comp+1] @ D[comp:comp+1,:]) / norm(X)\n",
    "    VAFs[comp] = 1-VAF\n",
    "\n",
    "print(f'{VAFs} \\n sums up to: {sum(VAFs)}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
